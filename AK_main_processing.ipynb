{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('geo_env': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7bf14925a434f6b7c958ecb24d261e8248047be1db188dfff6ca1cc92668254a"
   }
  },
  "interpreter": {
   "hash": "ba7df6755b32bab95c00c96c0cebd9d890b50759e401e4d74df893f436e34bc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Shoreline extraction from Landsat MNDWI images for Northern Alaska (1986-2021)\n",
    "\n",
    "This notebook extracts subpixel contours from MNDWI images, which have been processing on  Google Earth Engine (see: https://code.earthengine.google.com/9d450b3d0d9840a352cbd3ba6ddb6ca5) and quantifies coastline change along shore-perpendicular transects.\n",
    "\n",
    "Content of the notebook:\n",
    "\n",
    "1. Setup\n",
    "2. Download MNDWI rasters from Google Drive\n",
    "3. Reproject and quality check MNDWI rasters\n",
    "4. Subpixel contours\n",
    "5. Create minimum water extent polygon\n",
    "6. Create transects\n",
    "7. Calculate intersections between shorelines and transects\n",
    "8. Coastline Change Analysis\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Further notes on Alaska: \n",
    "+ Projection: (EPSG 3413 (WGS 84/ NSIDC Sea Ice Polar Stereographic North), projected coordinate system for polar research)\n",
    "+ Projection: EPSG 5936 (WGS 84/ Alaska Polar Stereographic), projected coordinate system \n",
    "+ Polar night: from Novermber 18 or 19 for 66 days\n",
    "+ Cloud cover: \"Utqiavik is one of the cloudiest places on Earth\", completely overcast for more than 50% of the day. 70% overcast for 62% of the time. Dense fog on 65 days per year in the summer months. (Maykut, Gary A.; Church, Phil E. (1973). Journal of Applied Meteorology. Department of Atmospheric Sciences, University of Washington. pp. 620â€“621.)\n",
    "+ Snow: \"Freezing and snowfall an occur during any month of the year\" (https://web.archive.org/web/20130119021827/http://climate.gi.alaska.edu/Stations/Arctic/Barrow.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Interesting data: \n",
    "+ VHS Alaska (50 cm): https://soa-dnr.maps.arcgis.com/home/webmap/viewer.html?webmap=13dd1ccf165845eea5db36465e7d565c\n",
    "+ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.| Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import glob \n",
    "import numpy as np \n",
    "import shapely as shp\n",
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import rasterio as rio \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats \n",
    "from skimage.filters import threshold_otsu\n",
    "# my coastline methods \n",
    "from coasty import postprocess, analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "data_dir = os.path.join(os.getcwd(),\"data/Alaska_north\") # path to data-folder with aux data\n",
    "plot_dir = os.path.join(os.getcwd(),\"figures/plots\")\n",
    "country_bounds_path = os.path.join(data_dir,\"AK_north_slope_bounds.geojson\") # path to country bounds\n",
    "transects_path = os.path.join(data_dir,\"AKn_transects_gov_2km_200m\")\n",
    "osm_sl_path = os.path.join(os.path.join(data_dir,\"AKn_osm_coastline.geojson\")) # path to reference shoreline\n",
    "buffer_path = os.path.join(os.path.join(data_dir,\"AKn_buffer_5km\"))\n",
    "tile_name = \"Barrow\"\n",
    "\n",
    "# Params\n",
    "export_folder = \"GEE_alaska\"       # folder on Google Drive with GEE images to download\n",
    "crs = \"EPSG:5936\"                   # coordinate system code of a projected crs \n",
    "min_length = 3000                   # min length of shoreline to keep [m]\n",
    "buffer_dist = 5000                  # buffer around reference shorelines to clip detected shorelines [m]\n",
    "transect_len = 3000                 # length of transects [m]\n",
    "transect_dist = 200                 # distance between transects [m]\n",
    "transect_min_line_length = 10000    # min legnth of polygon outline at which to draw transects [m]\n",
    "                                    # (for removing small islands) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Everything successfully read.\n"
     ]
    }
   ],
   "source": [
    "# read/ create aux data \n",
    "#country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "osm_sl = gpd.read_file(osm_sl_path).to_crs(crs)\n",
    "\n",
    "try:\n",
    "    buffer = gpd.read_file(buffer_path)\n",
    "    print(\"Everything successfully read.\")\n",
    "except:\n",
    "    print(\"Create osm shoreline buffer:\")\n",
    "    buffer = osm_sl.buffer(buffer_dist)\n",
    "    buffer.to_file(buffer_path,driver=\"GeoJSON\")\n",
    "    print(\"Buffer saved.\")"
   ]
  },
  {
   "source": [
    "### 2.| Download MNDWI rasters from Google Drive\n",
    "done manually in this case. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.| Reproject and quality check MNDWI rasters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Barrow --------------------\n",
      "Barrow_1985.tif reprojected.\n",
      "Barrow_1985.tif masked.\n",
      "Barrow_1985_01avg_aq.tif saved.\n",
      "Barrow_1985.tif removed.\n",
      "Barrow_1986.tif reprojected.\n",
      "Barrow_1986.tif masked.\n",
      "Barrow_1986_04avg_aq.tif saved.\n",
      "Barrow_1986.tif removed.\n",
      "Barrow_1987.tif reprojected.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/coasty/postprocess.py:86: RuntimeWarning: Mean of empty slice\n",
      "  avg_aq = int(np.nanmean(nobs))\n",
      "Barrow_1987.tif could not be masked\n",
      "Barrow_1990.tif reprojected.\n",
      "Barrow_1990.tif masked.\n",
      "Barrow_1990_01avg_aq.tif saved.\n",
      "Barrow_1990.tif removed.\n",
      "Barrow_1991.tif reprojected.\n",
      "Barrow_1991.tif could not be masked\n",
      "Barrow_1995.tif reprojected.\n",
      "Barrow_1995.tif masked.\n",
      "Barrow_1995_01avg_aq.tif saved.\n",
      "Barrow_1995.tif removed.\n",
      "Barrow_1999.tif reprojected.\n",
      "Barrow_1999.tif masked.\n",
      "Barrow_1999_02avg_aq.tif saved.\n",
      "Barrow_1999.tif removed.\n",
      "Barrow_2000.tif reprojected.\n",
      "Barrow_2000.tif masked.\n",
      "Barrow_2000_09avg_aq.tif saved.\n",
      "Barrow_2000.tif removed.\n",
      "Barrow_2001.tif reprojected.\n",
      "Barrow_2001.tif masked.\n",
      "Barrow_2001_09avg_aq.tif saved.\n",
      "Barrow_2001.tif removed.\n",
      "Barrow_2002.tif reprojected.\n",
      "Barrow_2002.tif masked.\n",
      "Barrow_2002_14avg_aq.tif saved.\n",
      "Barrow_2002.tif removed.\n",
      "Barrow_2003.tif reprojected.\n",
      "Barrow_2003.tif masked.\n",
      "Barrow_2003_02avg_aq.tif saved.\n",
      "Barrow_2003.tif removed.\n",
      "Barrow_2004.tif reprojected.\n",
      "Barrow_2004.tif masked.\n",
      "Barrow_2004_09avg_aq.tif saved.\n",
      "Barrow_2004.tif removed.\n",
      "Barrow_2005.tif reprojected.\n",
      "Barrow_2005.tif masked.\n",
      "Barrow_2005_08avg_aq.tif saved.\n",
      "Barrow_2005.tif removed.\n",
      "Barrow_2006.tif reprojected.\n",
      "Barrow_2006.tif masked.\n",
      "Barrow_2006_07avg_aq.tif saved.\n",
      "Barrow_2006.tif removed.\n",
      "Barrow_2007.tif reprojected.\n",
      "Barrow_2007.tif masked.\n",
      "Barrow_2007_06avg_aq.tif saved.\n",
      "Barrow_2007.tif removed.\n",
      "Barrow_2008.tif reprojected.\n",
      "Barrow_2008.tif masked.\n",
      "Barrow_2008_12avg_aq.tif saved.\n",
      "Barrow_2008.tif removed.\n",
      "Barrow_2009.tif reprojected.\n",
      "Barrow_2009.tif masked.\n",
      "Barrow_2009_09avg_aq.tif saved.\n",
      "Barrow_2009.tif removed.\n",
      "Barrow_2010.tif reprojected.\n",
      "Barrow_2010.tif masked.\n",
      "Barrow_2010_08avg_aq.tif saved.\n",
      "Barrow_2010.tif removed.\n",
      "Barrow_2011.tif reprojected.\n",
      "Barrow_2011.tif masked.\n",
      "Barrow_2011_14avg_aq.tif saved.\n",
      "Barrow_2011.tif removed.\n",
      "Barrow_2012.tif reprojected.\n",
      "Barrow_2012.tif masked.\n",
      "Barrow_2012_06avg_aq.tif saved.\n",
      "Barrow_2012.tif removed.\n",
      "Barrow_2013.tif reprojected.\n",
      "Barrow_2013.tif masked.\n",
      "Barrow_2013_09avg_aq.tif saved.\n",
      "Barrow_2013.tif removed.\n",
      "Barrow_2014.tif reprojected.\n",
      "Barrow_2014.tif masked.\n",
      "Barrow_2014_10avg_aq.tif saved.\n",
      "Barrow_2014.tif removed.\n",
      "Barrow_2015_11avg_aq.tif already exists.\n",
      "Barrow_2015_11avg_aq.tif already projected to given CRS.\n",
      "Barrow_2015_11avg_aq.tif could not be masked\n",
      "Barrow_2016.tif reprojected.\n",
      "Barrow_2016.tif masked.\n",
      "Barrow_2016_13avg_aq.tif saved.\n",
      "Barrow_2016.tif removed.\n",
      "Barrow_2017.tif reprojected.\n",
      "Barrow_2017.tif masked.\n",
      "Barrow_2017_15avg_aq.tif saved.\n",
      "Barrow_2017.tif removed.\n",
      "Barrow_2018.tif reprojected.\n",
      "Barrow_2018.tif masked.\n",
      "Barrow_2018_17avg_aq.tif saved.\n",
      "Barrow_2018.tif removed.\n",
      "Barrow_2019.tif reprojected.\n",
      "Barrow_2019.tif masked.\n",
      "Barrow_2019_12avg_aq.tif saved.\n",
      "Barrow_2019.tif removed.\n",
      "Barrow_2020.tif reprojected.\n",
      "Barrow_2020.tif masked.\n",
      "Barrow_2020_12avg_aq.tif saved.\n",
      "Barrow_2020.tif removed.\n"
     ]
    }
   ],
   "source": [
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "raster_paths = glob.glob(os.path.join(data_dir,tile_name,tile_name+\"_*.tif\"))\n",
    "raster_paths.sort()\n",
    "for r in raster_paths:\n",
    "    masked_file = glob.glob(r[:-4]+\"*aq.tif\") \n",
    "    if len(masked_file) == 0:\n",
    "        postprocess.reproject_raster(r,r,crs)\n",
    "        try:\n",
    "            postprocess.mask_single_observation_pixel(r)\n",
    "            os.remove(r)\n",
    "            print(os.path.basename(r),\"removed.\")\n",
    "        except:\n",
    "            print(os.path.basename(r),'could not be masked')\n",
    "            pass\n",
    "    else:\n",
    "        print(os.path.basename(masked_file[0]), \"already exists.\")"
   ]
  },
  {
   "source": [
    "### 4.| Subpixel contours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Barrow --------------------\n",
      "Process shorelines...\n",
      "0.21091056\n",
      "1985: shoreline processed.\n",
      "0.46515965\n",
      "1986: shoreline processed.\n",
      "1990: shoreline processed.\n",
      "0.1778094\n",
      "1995: shoreline processed.\n",
      "0.22073612\n",
      "1999: shoreline processed.\n",
      "0.38250306\n",
      "2000: shoreline processed.\n",
      "0.8308344\n",
      "2001: shoreline processed.\n",
      "0.8407794\n",
      "2002: shoreline processed.\n",
      "0.24962574\n",
      "2003: shoreline processed.\n",
      "0.87164164\n",
      "2004: shoreline processed.\n",
      "0.3492409\n",
      "2005: shoreline processed.\n",
      "0.31950837\n",
      "2006: shoreline processed.\n",
      "0.31681895\n",
      "2007: shoreline processed.\n",
      "0.34584063\n",
      "2008: shoreline processed.\n",
      "0.41513437\n",
      "2009: shoreline processed.\n",
      "0.43543744\n",
      "2010: shoreline processed.\n",
      "0.86784446\n",
      "2011: shoreline processed.\n",
      "0.2945012\n",
      "2012: shoreline processed.\n",
      "0.3619105\n",
      "2013: shoreline processed.\n",
      "0.37599027\n",
      "2014: shoreline processed.\n",
      "0.30046397\n",
      "2015: shoreline processed.\n",
      "0.55121434\n",
      "2016: shoreline processed.\n",
      "0.51456934\n",
      "2017: shoreline processed.\n",
      "0.8831848\n",
      "2018: shoreline processed.\n",
      "0.4468025\n",
      "2019: shoreline processed.\n",
      "0.47132164\n",
      "2020: shoreline processed.\n",
      "All shorelines have been created and saved.\n",
      "CPU times: user 1h 2min 33s, sys: 2min 1s, total: 1h 4min 34s\n",
      "Wall time: 1h 13min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "# Create shorelines\n",
    "shorelines_path = os.path.join(folder_path,tile_name+\"_shorelines\") \n",
    "if not os.path.exists(shorelines_path):\n",
    "    shorelines = []\n",
    "    print(\"Process shorelines...\")    \n",
    "    raster_paths = glob.glob(os.path.join(data_dir,tile_name,\"*aq.tif\"))\n",
    "    raster_paths.sort()\n",
    "    for r in raster_paths:\n",
    "        # create path for single shorelines \n",
    "        sl_folder_path = os.path.join(folder_path,tile_name+\"_single_shorelines\")\n",
    "        sl_path = os.path.join(sl_folder_path,os.path.splitext(os.path.basename(r))[0]+\"_shoreline\")\n",
    "        # save single shoreline without modifications as backup\n",
    "        if not os.path.exists(sl_path):\n",
    "            with rio.open(r,\"r\") as raster:\n",
    "                mndwi = raster.read(1)\n",
    "                if np.count_nonzero(mndwi) > 0 and np.count_nonzero(~np.isnan(mndwi)) > 0:                    \n",
    "                    thres = threshold_otsu(mndwi[~np.isnan(mndwi)])\n",
    "                    print(thres)\n",
    "                    shoreline = postprocess.subpixel_contours(r,thres)\n",
    "                    if not shoreline.empty:\n",
    "                        if not os.path.exists(sl_folder_path): os.mkdir(sl_folder_path)\n",
    "                        shoreline.to_file(os.path.join(sl_path),driver=\"GeoJSON\")\n",
    "        else:\n",
    "            shoreline = gpd.read_file(sl_path)\n",
    "        # postprocess raw shorelines\n",
    "        shoreline = gpd.clip(shoreline,buffer)\n",
    "        cleaned = postprocess.remove_small_lines(shoreline, min_size=min_length)\n",
    "        if not cleaned.empty:\n",
    "            year = os.path.basename(r)[7:11]\n",
    "            avg_aq = os.path.basename(r)[12:14]\n",
    "            cleaned['id']=year\n",
    "            cleaned = cleaned.dissolve(by=cleaned.id,aggfunc=\"sum\")\n",
    "            cleaned['year']=year\n",
    "            cleaned['avg_aq']=avg_aq\n",
    "            cleaned['otsu_thres']=str(thres)\n",
    "            shorelines.append(cleaned)\n",
    "            print(year+\": shoreline processed.\")\n",
    "    shorelines_gdf = pd.concat(shorelines,ignore_index=True)    \n",
    "    shorelines_gdf.to_file(os.path.join(shorelines_path),driver=\"GPKG\")\n",
    "    print(\"All shorelines have been created and saved.\")\n",
    "else:\n",
    "    print(\"Shorelines already exist.\")"
   ]
  },
  {
   "source": [
    "### 5.| Create minimum water extent polygon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Generate minimum and maximum water extent raster for each processing tile"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Barrow --------------------\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2016_13avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2017_15avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2000_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2008_12avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1986_04avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2018_17avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2014_10avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2013_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2001_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1985_01avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2002_14avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2015_11avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1999_02avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2020_12avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2011_14avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2012_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2007_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2009_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2005_08avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2010_08avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1995_01avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2019_12avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2004_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2006_07avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2003_02avg_aq_bin.tif saved.\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1986_04avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2009_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1995_01avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2005_08avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2018_17avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1985_01avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2010_08avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2015_11avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2016_13avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2006_07avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2020_12avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2002_14avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2013_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2019_12avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2000_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2011_14avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2012_06avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2004_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2001_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2003_02avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2017_15avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_1999_02avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2014_10avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2008_12avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Alaska_north/Barrow/Barrow_2007_06avg_aq_bin.tif\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/coasty/analysis.py:26: RuntimeWarning: All-NaN slice encountered\n",
      "  min_water_extent = np.nanmin(all_masks, 0)  # water = 1, min water extent\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/coasty/analysis.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "  max_water_extent = np.nanmax(all_masks, 0)  # no water = 0, max water extent\n"
     ]
    }
   ],
   "source": [
    "# Calculate raster with min and max water extent \n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    raster_paths = glob.glob(os.path.join(folder_path,\"*aq.tif\"))\n",
    "    # make MNDWI images binary first\n",
    "    # (this step should later be included to the shoreline extraction script, where the Otsu is already being calculated) \n",
    "    for r in raster_paths:\n",
    "        # create binary raster using the Otsu threshold for min water raster                \n",
    "        binary_file = os.path.join(folder_path,os.path.splitext(os.path.basename(r))[0]+\"_bin.tif\")\n",
    "        if not os.path.exists(binary_file):\n",
    "            with rio.open(r,\"r\") as raster:\n",
    "                mndwi = raster.read(1)\n",
    "                if np.count_nonzero(mndwi) > 0 and np.count_nonzero(~np.isnan(mndwi)) > 0:                    \n",
    "                    meta = raster.meta\n",
    "                    thres = threshold_otsu(mndwi[~np.isnan(mndwi)])\n",
    "                    binary = mndwi.copy()\n",
    "                    binary[binary > thres] = 1\n",
    "                    binary[binary < thres] = 0\n",
    "                    meta.update({\n",
    "                        \"compress\":\"LZW\",\n",
    "                        })\n",
    "                    with rio.open(binary_file,'w',**meta) as dst:\n",
    "                        dst.write(binary,1)\n",
    "                    print(binary_file, \"saved.\")\n",
    "        else:\n",
    "            print(binary_file, \"exists.\")\n",
    "    binary_paths = glob.glob(os.path.join(folder_path,\"*aq_bin.tif\"))\n",
    "    min_water_file = os.path.join(data_dir,tile_name,tile_name+\"_min_water_extent\")\n",
    "    max_water_file = os.path.join(data_dir,tile_name,tile_name+\"_max_water_extent\")\n",
    "    if not os.path.exists(min_water_file):\n",
    "        analysis.calc_water_extent(binary_paths,min_water_file,max_water_file)\n",
    "    else:\n",
    "        print(\"Files exist.\")"
   ]
  },
  {
   "source": [
    "Generalize, vectorize and merge minimum water extent rasters and create transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Barrow --------------------\n",
      "Pixel cluster removed.\n",
      "Minimum water extent polygon created.\n",
      "\n",
      "CPU times: user 28 s, sys: 7.85 s, total: 35.9 s\n",
      "Wall time: 46.7 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'    \\n    min_water_poly = gpd.read_file(os.path.join(folder_path,tile_name+\"_min_water_extent_simple_poly\"))\\n    min_water_polys.append(min_water_poly)\\n# Concatenate all polygons\\nmin_water_polys_gdf = pd.concat(min_water_polys,ignore_index=True)\\n# Dissolve overlapping polygons\\ngeoms = min_water_polys_gdf.geometry.unary_union\\nmin_water_polys_gdf = gpd.GeoDataFrame(geometry=[geoms],crs=crs)\\nmin_water_polys_gdf = min_water_polys_gdf.explode().reset_index(drop=True)\\nmin_water_polys_gdf.to_file(all_min_water_polys_file,driver=\"GeoJSON\")\\nprint(\"Minimum water extent raster for Vietnam has been saved.\")'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "%%time\n",
    "#remove small pixel cluster in min and max water extent rasters and merge rasters of all tiles\n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "# define path for generalized min water extent raster \n",
    "min_water_simple_file = os.path.join(data_dir,tile_name,tile_name+\"_min_water_extent_simple\")\n",
    "if not os.path.exists(min_water_simple_file+\"_poly\"):\n",
    "    try:\n",
    "        # read min water extent file\n",
    "        min_water_file = os.path.join(folder_path,tile_name+\"_min_water_extent\")\n",
    "    except FileNotFoundError:\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        # remove small objects from raster\n",
    "        analysis.remove_pixel_cluster(min_water_file,min_water_simple_file,50000,100000,0)\n",
    "        print(\"Pixel cluster removed.\")\n",
    "        # vectorize raster \n",
    "        min_water_poly = analysis.vectorize_raster(min_water_simple_file,0)\n",
    "        if not min_water_poly.empty:\n",
    "            min_water_poly.to_file(min_water_simple_file+\"_poly\",driver=\"GeoJSON\")\n",
    "            print(\"Minimum water extent polygon created.\\n\")\n",
    "else:\n",
    "    print(\"Minimum water extent polygon already exists.\\n\")\n"
   ]
  },
  {
   "source": [
    "### 6.| Create transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create transects...\n",
      "(0, 0) n_points: 13477\n",
      "Transects have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "country_bounds.geometry = country_bounds.buffer(-6500)\n",
    "\n",
    "try:\n",
    "    transects = gpd.read_file(transects_path)\n",
    "    print(\"Transects exist and have been loaded.\")\n",
    "except:\n",
    "    print(\"Create transects...\")\n",
    "    country_bounds = country_bounds.explode()\n",
    "    # only take the land polygon to exclude islands etc.\n",
    "    country_bounds['area'] = country_bounds.geometry.area\n",
    "    country_bounds = country_bounds[country_bounds.area == np.max(country_bounds.area)]\n",
    "    #country_bounds_gov.geometry = country_bounds_gov.geometry.simplify(500,preserve_topology=True)\n",
    "    country_bounds.to_file(country_bounds_path+\"_simple\",driver=\"GeoJSON\")\n",
    "    # draw transects at country polygon \n",
    "    transects = postprocess.draw_transects_polygon(\n",
    "        country_bounds,\n",
    "        transect_len/2,\n",
    "        transect_len/2,\n",
    "        transect_dist,\n",
    "        transect_min_line_length,\n",
    "        sigma=3,\n",
    "        out_path_poly=country_bounds_path+\"_smooth\"\n",
    "        )\n",
    "    # clip transects to buffer \n",
    "    transects = gpd.clip(transects,buffer)\n",
    "    transects = transects.dropna() # if transects have been created along a multipolygon\n",
    "    transects = transects.explode().reset_index(drop=True)\n",
    "    transects.to_file(transects_path,driver=\"GeoJSON\")\n",
    "    print(\"Transects have been created and saved.\")\n",
    "else:\n",
    "    # clip transects to min water extent raster\n",
    "    if not os.path.exists(transects_path+\"_clip\"):\n",
    "        print(\"Clip transects to min water extent...\")\n",
    "        min_water_buffer = min_water_poly.buffer(100)\n",
    "        transects_clip = gpd.clip(transects,min_water_buffer)\n",
    "        # convert all mutlilinestrings to single linestrings to treat transect pieces separately\n",
    "        transects_clip = transects_clip.explode().reset_index()\n",
    "        transects_clip.to_file(transects_path+\"_clip\",driver=\"GPKG\")\n",
    "        print(\"Transects haven been clipped to min water extent polygon and saved.\")\n",
    "    else:\n",
    "        print(\"Clipped transects already exist.\")"
   ]
  },
  {
   "source": [
    "### 7.| Calculate intersections between shorelines and transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "662 intersected\n",
      "5663 intersected\n",
      "5664 intersected\n",
      "5665 intersected\n",
      "5666 intersected\n",
      "5667 intersected\n",
      "5668 intersected\n",
      "5669 intersected\n",
      "5670 intersected\n",
      "5671 intersected\n",
      "5672 intersected\n",
      "5673 intersected\n",
      "5674 intersected\n",
      "5675 intersected\n",
      "5676 intersected\n",
      "5677 intersected\n",
      "5678 intersected\n",
      "5679 intersected\n",
      "5680 intersected\n",
      "5681 intersected\n",
      "5682 intersected\n",
      "5683 intersected\n",
      "5684 intersected\n",
      "5685 intersected\n",
      "5686 intersected\n",
      "5687 intersected\n",
      "5688 intersected\n",
      "5689 intersected\n",
      "5690 intersected\n",
      "5691 intersected\n",
      "5692 intersected\n",
      "5693 intersected\n",
      "5694 intersected\n",
      "5695 intersected\n",
      "5696 intersected\n",
      "5697 intersected\n",
      "5698 intersected\n",
      "5699 intersected\n",
      "5700 intersected\n",
      "5701 intersected\n",
      "5702 intersected\n",
      "5703 intersected\n",
      "5704 intersected\n",
      "5705 intersected\n",
      "5706 intersected\n",
      "5707 intersected\n",
      "5708 intersected\n",
      "5709 intersected\n",
      "5710 intersected\n",
      "5711 intersected\n",
      "5712 intersected\n",
      "5713 intersected\n",
      "5714 intersected\n",
      "5715 intersected\n",
      "5716 intersected\n",
      "5717 intersected\n",
      "5718 intersected\n",
      "5719 intersected\n",
      "5720 intersected\n",
      "5721 intersected\n",
      "5722 intersected\n",
      "5723 intersected\n",
      "5724 intersected\n",
      "5725 intersected\n",
      "5726 intersected\n",
      "5727 intersected\n",
      "5728 intersected\n",
      "5729 intersected\n",
      "5730 intersected\n",
      "5731 intersected\n",
      "5732 intersected\n",
      "5733 intersected\n",
      "5734 intersected\n",
      "5735 intersected\n",
      "5736 intersected\n",
      "5737 intersected\n",
      "5738 intersected\n",
      "5739 intersected\n",
      "5740 intersected\n",
      "5741 intersected\n",
      "5742 intersected\n",
      "5743 intersected\n",
      "5744 intersected\n",
      "5745 intersected\n",
      "5746 intersected\n",
      "5747 intersected\n",
      "5748 intersected\n",
      "5749 intersected\n",
      "5750 intersected\n",
      "5751 intersected\n",
      "5752 intersected\n",
      "5753 intersected\n",
      "5754 intersected\n",
      "5755 intersected\n",
      "5756 intersected\n",
      "5757 intersected\n",
      "5758 intersected\n",
      "5759 intersected\n",
      "5760 intersected\n",
      "5761 intersected\n",
      "5762 intersected\n",
      "5763 intersected\n",
      "5764 intersected\n",
      "5765 intersected\n",
      "5766 intersected\n",
      "5767 intersected\n",
      "5768 intersected\n",
      "5769 intersected\n",
      "5770 intersected\n",
      "5771 intersected\n",
      "5772 intersected\n",
      "5773 intersected\n",
      "5774 intersected\n",
      "5775 intersected\n",
      "5776 intersected\n",
      "5777 intersected\n",
      "5778 intersected\n",
      "5779 intersected\n",
      "5780 intersected\n",
      "5781 intersected\n",
      "5782 intersected\n",
      "5783 intersected\n",
      "5784 intersected\n",
      "5785 intersected\n",
      "5786 intersected\n",
      "5787 intersected\n",
      "5788 intersected\n",
      "5789 intersected\n",
      "5790 intersected\n",
      "5791 intersected\n",
      "5792 intersected\n",
      "5793 intersected\n",
      "5794 intersected\n",
      "5795 intersected\n",
      "5796 intersected\n",
      "5797 intersected\n",
      "5798 intersected\n",
      "5799 intersected\n",
      "5800 intersected\n",
      "5801 intersected\n",
      "5802 intersected\n",
      "5803 intersected\n",
      "5804 intersected\n",
      "5805 intersected\n",
      "5806 intersected\n",
      "5807 intersected\n",
      "5808 intersected\n",
      "5809 intersected\n",
      "5810 intersected\n",
      "5811 intersected\n",
      "5812 intersected\n",
      "5813 intersected\n",
      "5814 intersected\n",
      "5815 intersected\n",
      "5816 intersected\n",
      "5817 intersected\n",
      "5818 intersected\n",
      "5819 intersected\n",
      "5820 intersected\n",
      "5821 intersected\n",
      "5822 intersected\n",
      "5823 intersected\n",
      "5824 intersected\n",
      "5825 intersected\n",
      "5826 intersected\n",
      "5827 intersected\n",
      "5828 intersected\n",
      "5829 intersected\n",
      "5830 intersected\n",
      "5831 intersected\n",
      "5832 intersected\n",
      "5833 intersected\n",
      "5834 intersected\n",
      "5835 intersected\n",
      "5836 intersected\n",
      "5837 intersected\n",
      "5838 intersected\n",
      "5839 intersected\n",
      "5840 intersected\n",
      "5841 intersected\n",
      "5842 intersected\n",
      "5843 intersected\n",
      "5844 intersected\n",
      "5845 intersected\n",
      "5846 intersected\n",
      "5847 intersected\n",
      "5848 intersected\n",
      "5849 intersected\n",
      "5850 intersected\n",
      "5851 intersected\n",
      "5852 intersected\n",
      "5853 intersected\n",
      "5854 intersected\n",
      "5855 intersected\n",
      "5856 intersected\n",
      "5857 intersected\n",
      "5858 intersected\n",
      "5859 intersected\n",
      "5860 intersected\n",
      "5861 intersected\n",
      "5862 intersected\n",
      "5863 intersected\n",
      "5864 intersected\n",
      "5865 intersected\n",
      "5866 intersected\n",
      "5867 intersected\n",
      "5868 intersected\n",
      "5869 intersected\n",
      "5870 intersected\n",
      "5871 intersected\n",
      "5872 intersected\n",
      "5873 intersected\n",
      "5874 intersected\n",
      "5875 intersected\n",
      "5876 intersected\n",
      "5877 intersected\n",
      "5878 intersected\n",
      "5879 intersected\n",
      "5880 intersected\n",
      "5881 intersected\n",
      "5882 intersected\n",
      "5883 intersected\n",
      "5884 intersected\n",
      "5885 intersected\n",
      "5886 intersected\n",
      "5887 intersected\n",
      "5888 intersected\n",
      "5889 intersected\n",
      "5890 intersected\n",
      "5891 intersected\n",
      "5892 intersected\n",
      "5893 intersected\n",
      "5894 intersected\n",
      "5895 intersected\n",
      "5896 intersected\n",
      "5897 intersected\n",
      "5898 intersected\n",
      "5899 intersected\n",
      "5900 intersected\n",
      "5901 intersected\n",
      "5902 intersected\n",
      "5903 intersected\n",
      "5904 intersected\n",
      "5905 intersected\n",
      "5906 intersected\n",
      "5907 intersected\n",
      "5908 intersected\n",
      "5909 intersected\n",
      "5910 intersected\n",
      "5911 intersected\n",
      "5912 intersected\n",
      "5913 intersected\n",
      "5914 intersected\n",
      "5915 intersected\n",
      "5916 intersected\n",
      "5917 intersected\n",
      "5918 intersected\n",
      "5919 intersected\n",
      "5920 intersected\n",
      "5921 intersected\n",
      "5922 intersected\n",
      "5923 intersected\n",
      "5924 intersected\n",
      "5925 intersected\n",
      "5926 intersected\n",
      "5927 intersected\n",
      "5928 intersected\n",
      "5929 intersected\n",
      "5930 intersected\n",
      "5931 intersected\n",
      "5932 intersected\n",
      "5933 intersected\n",
      "5934 intersected\n",
      "5935 intersected\n",
      "5936 intersected\n",
      "5937 intersected\n",
      "5938 intersected\n",
      "5939 intersected\n",
      "5940 intersected\n",
      "5941 intersected\n",
      "5942 intersected\n",
      "5943 intersected\n",
      "5944 intersected\n",
      "5945 intersected\n",
      "5946 intersected\n",
      "5947 intersected\n",
      "5948 intersected\n",
      "5949 intersected\n",
      "5950 intersected\n",
      "5951 intersected\n",
      "5952 intersected\n",
      "5953 intersected\n",
      "5954 intersected\n",
      "5955 intersected\n",
      "5956 intersected\n",
      "5957 intersected\n",
      "5958 intersected\n",
      "5959 intersected\n",
      "5960 intersected\n",
      "5961 intersected\n",
      "5962 intersected\n",
      "5963 intersected\n",
      "5964 intersected\n",
      "5965 intersected\n",
      "5966 intersected\n",
      "5967 intersected\n",
      "5968 intersected\n",
      "5969 intersected\n",
      "5970 intersected\n",
      "5971 intersected\n",
      "5972 intersected\n",
      "5973 intersected\n",
      "5974 intersected\n",
      "5975 intersected\n",
      "5976 intersected\n",
      "5977 intersected\n",
      "5978 intersected\n",
      "5979 intersected\n",
      "5980 intersected\n",
      "5981 intersected\n",
      "5982 intersected\n",
      "5983 intersected\n",
      "5984 intersected\n",
      "5985 intersected\n",
      "5986 intersected\n",
      "5987 intersected\n",
      "5988 intersected\n",
      "5989 intersected\n",
      "5990 intersected\n",
      "5991 intersected\n",
      "5992 intersected\n",
      "5993 intersected\n",
      "5994 intersected\n",
      "5995 intersected\n",
      "5996 intersected\n",
      "5997 intersected\n",
      "5998 intersected\n",
      "5999 intersected\n",
      "6000 intersected\n",
      "6001 intersected\n",
      "6002 intersected\n",
      "6003 intersected\n",
      "6004 intersected\n",
      "6005 intersected\n",
      "6006 intersected\n",
      "6007 intersected\n",
      "6008 intersected\n",
      "6009 intersected\n",
      "6010 intersected\n",
      "6011 intersected\n",
      "6012 intersected\n",
      "6013 intersected\n",
      "6014 intersected\n",
      "6015 intersected\n",
      "6016 intersected\n",
      "6017 intersected\n",
      "6018 intersected\n",
      "6019 intersected\n",
      "6020 intersected\n",
      "6021 intersected\n",
      "6022 intersected\n",
      "6023 intersected\n",
      "6024 intersected\n",
      "6025 intersected\n",
      "6026 intersected\n",
      "6027 intersected\n",
      "6028 intersected\n",
      "6029 intersected\n",
      "6030 intersected\n",
      "6031 intersected\n",
      "6032 intersected\n",
      "6033 intersected\n",
      "6034 intersected\n",
      "6035 intersected\n",
      "6036 intersected\n",
      "6037 intersected\n",
      "6038 intersected\n",
      "6039 intersected\n",
      "6040 intersected\n",
      "6041 intersected\n",
      "6042 intersected\n",
      "6043 intersected\n",
      "6044 intersected\n",
      "6045 intersected\n",
      "6046 intersected\n",
      "6047 intersected\n",
      "6048 intersected\n",
      "6049 intersected\n",
      "6050 intersected\n",
      "6051 intersected\n",
      "6052 intersected\n",
      "6053 intersected\n",
      "6054 intersected\n",
      "6055 intersected\n",
      "6056 intersected\n",
      "6057 intersected\n",
      "6058 intersected\n",
      "6059 intersected\n",
      "6060 intersected\n",
      "6061 intersected\n",
      "6062 intersected\n",
      "6063 intersected\n",
      "6064 intersected\n",
      "6065 intersected\n",
      "6066 intersected\n",
      "6067 intersected\n",
      "6068 intersected\n",
      "6069 intersected\n",
      "6070 intersected\n",
      "6071 intersected\n",
      "6072 intersected\n",
      "6073 intersected\n",
      "6074 intersected\n",
      "6075 intersected\n",
      "6076 intersected\n",
      "6077 intersected\n",
      "6078 intersected\n",
      "6079 intersected\n",
      "6080 intersected\n",
      "6081 intersected\n",
      "6082 intersected\n",
      "6083 intersected\n",
      "6084 intersected\n",
      "6085 intersected\n",
      "6086 intersected\n",
      "6087 intersected\n",
      "6088 intersected\n",
      "6089 intersected\n",
      "6090 intersected\n",
      "6091 intersected\n",
      "6092 intersected\n",
      "6093 intersected\n",
      "6094 intersected\n",
      "6095 intersected\n",
      "6096 intersected\n",
      "6097 intersected\n",
      "6098 intersected\n",
      "6099 intersected\n",
      "6100 intersected\n",
      "6101 intersected\n",
      "6102 intersected\n",
      "6103 intersected\n",
      "6104 intersected\n",
      "6105 intersected\n",
      "6106 intersected\n",
      "6107 intersected\n",
      "6108 intersected\n",
      "6109 intersected\n",
      "6110 intersected\n",
      "6111 intersected\n",
      "6112 intersected\n",
      "6113 intersected\n",
      "6114 intersected\n",
      "6115 intersected\n",
      "6116 intersected\n",
      "6117 intersected\n",
      "6118 intersected\n",
      "6119 intersected\n",
      "6120 intersected\n",
      "6121 intersected\n",
      "6122 intersected\n",
      "6123 intersected\n",
      "6124 intersected\n",
      "6125 intersected\n",
      "6126 intersected\n",
      "6127 intersected\n",
      "6128 intersected\n",
      "6129 intersected\n",
      "6130 intersected\n",
      "6131 intersected\n",
      "6132 intersected\n",
      "6133 intersected\n",
      "6134 intersected\n",
      "6135 intersected\n",
      "6136 intersected\n",
      "6137 intersected\n",
      "6138 intersected\n",
      "6139 intersected\n",
      "6140 intersected\n",
      "6141 intersected\n",
      "6142 intersected\n",
      "6143 intersected\n",
      "6144 intersected\n",
      "6145 intersected\n",
      "6146 intersected\n",
      "6147 intersected\n",
      "6148 intersected\n",
      "6149 intersected\n",
      "6150 intersected\n",
      "6151 intersected\n",
      "6152 intersected\n",
      "6153 intersected\n",
      "6154 intersected\n",
      "6155 intersected\n",
      "6156 intersected\n",
      "6157 intersected\n",
      "6158 intersected\n",
      "6159 intersected\n",
      "6160 intersected\n",
      "6161 intersected\n",
      "6162 intersected\n",
      "6163 intersected\n",
      "6164 intersected\n",
      "6165 intersected\n",
      "6166 intersected\n",
      "6167 intersected\n",
      "6168 intersected\n",
      "6169 intersected\n",
      "6170 intersected\n",
      "6171 intersected\n",
      "6172 intersected\n",
      "6173 intersected\n",
      "6174 intersected\n",
      "6175 intersected\n",
      "6176 intersected\n",
      "6177 intersected\n",
      "6178 intersected\n",
      "6179 intersected\n",
      "6180 intersected\n",
      "6181 intersected\n",
      "6182 intersected\n",
      "6183 intersected\n",
      "6184 intersected\n",
      "6185 intersected\n",
      "6186 intersected\n",
      "6187 intersected\n",
      "6188 intersected\n",
      "6189 intersected\n",
      "6190 intersected\n",
      "6191 intersected\n",
      "6192 intersected\n",
      "6193 intersected\n",
      "6194 intersected\n",
      "6195 intersected\n",
      "6196 intersected\n",
      "6197 intersected\n",
      "6198 intersected\n",
      "6199 intersected\n",
      "6200 intersected\n",
      "6201 intersected\n",
      "6202 intersected\n",
      "6203 intersected\n",
      "6204 intersected\n",
      "6205 intersected\n",
      "6206 intersected\n",
      "6207 intersected\n",
      "6208 intersected\n",
      "6209 intersected\n",
      "6210 intersected\n",
      "6211 intersected\n",
      "6212 intersected\n",
      "6213 intersected\n",
      "6214 intersected\n",
      "6215 intersected\n",
      "6216 intersected\n",
      "6217 intersected\n",
      "6218 intersected\n",
      "6219 intersected\n",
      "6220 intersected\n",
      "6221 intersected\n",
      "6222 intersected\n",
      "6223 intersected\n",
      "6224 intersected\n",
      "6225 intersected\n",
      "6226 intersected\n",
      "6227 intersected\n",
      "6228 intersected\n",
      "6229 intersected\n",
      "6230 intersected\n",
      "6231 intersected\n",
      "6232 intersected\n",
      "6233 intersected\n",
      "6234 intersected\n",
      "6235 intersected\n",
      "6236 intersected\n",
      "6237 intersected\n",
      "6238 intersected\n",
      "6239 intersected\n",
      "6240 intersected\n",
      "6241 intersected\n",
      "6242 intersected\n",
      "6243 intersected\n",
      "6244 intersected\n",
      "6245 intersected\n",
      "6246 intersected\n",
      "6247 intersected\n",
      "6248 intersected\n",
      "6249 intersected\n",
      "6250 intersected\n",
      "6251 intersected\n",
      "6252 intersected\n",
      "6253 intersected\n",
      "6254 intersected\n",
      "6255 intersected\n",
      "6256 intersected\n",
      "6257 intersected\n",
      "6258 intersected\n",
      "6259 intersected\n",
      "6260 intersected\n",
      "6261 intersected\n",
      "6262 intersected\n",
      "6263 intersected\n",
      "6264 intersected\n",
      "6265 intersected\n",
      "6266 intersected\n",
      "6267 intersected\n",
      "6268 intersected\n",
      "6269 intersected\n",
      "6270 intersected\n",
      "6271 intersected\n",
      "6272 intersected\n",
      "6273 intersected\n",
      "6274 intersected\n",
      "6275 intersected\n",
      "6276 intersected\n",
      "6277 intersected\n",
      "6278 intersected\n",
      "6279 intersected\n",
      "6280 intersected\n",
      "6281 intersected\n",
      "6282 intersected\n",
      "6283 intersected\n",
      "6284 intersected\n",
      "6285 intersected\n",
      "6286 intersected\n",
      "6287 intersected\n",
      "6288 intersected\n",
      "6289 intersected\n",
      "6290 intersected\n",
      "6291 intersected\n",
      "6292 intersected\n",
      "6293 intersected\n",
      "6294 intersected\n",
      "6295 intersected\n",
      "6296 intersected\n",
      "6297 intersected\n",
      "6298 intersected\n",
      "6299 intersected\n",
      "6300 intersected\n",
      "6301 intersected\n",
      "6302 intersected\n",
      "6303 intersected\n",
      "6304 intersected\n",
      "6305 intersected\n",
      "6306 intersected\n",
      "6307 intersected\n",
      "6308 intersected\n",
      "6309 intersected\n",
      "6310 intersected\n",
      "6311 intersected\n",
      "6312 intersected\n",
      "6313 intersected\n",
      "6314 intersected\n",
      "6315 intersected\n",
      "6316 intersected\n",
      "6317 intersected\n",
      "6318 intersected\n",
      "6319 intersected\n",
      "6320 intersected\n",
      "6321 intersected\n",
      "6322 intersected\n",
      "6323 intersected\n",
      "6324 intersected\n",
      "6325 intersected\n",
      "6326 intersected\n",
      "6327 intersected\n",
      "6328 intersected\n",
      "6329 intersected\n",
      "6330 intersected\n",
      "6331 intersected\n",
      "6332 intersected\n",
      "6333 intersected\n",
      "6334 intersected\n",
      "6335 intersected\n",
      "6336 intersected\n",
      "6337 intersected\n",
      "6338 intersected\n",
      "6339 intersected\n",
      "6340 intersected\n",
      "6341 intersected\n",
      "6342 intersected\n",
      "6343 intersected\n",
      "6344 intersected\n",
      "6345 intersected\n",
      "6346 intersected\n",
      "6347 intersected\n",
      "6348 intersected\n",
      "6349 intersected\n",
      "6350 intersected\n",
      "6351 intersected\n",
      "6352 intersected\n",
      "6353 intersected\n",
      "6354 intersected\n",
      "6355 intersected\n",
      "6356 intersected\n",
      "6357 intersected\n",
      "6358 intersected\n",
      "6359 intersected\n",
      "6360 intersected\n",
      "6361 intersected\n",
      "6362 intersected\n",
      "6363 intersected\n",
      "6364 intersected\n",
      "6365 intersected\n",
      "6366 intersected\n",
      "6367 intersected\n",
      "6368 intersected\n",
      "6369 intersected\n",
      "6370 intersected\n",
      "6371 intersected\n",
      "6372 intersected\n",
      "6373 intersected\n",
      "6374 intersected\n",
      "6375 intersected\n",
      "6376 intersected\n",
      "6377 intersected\n",
      "6378 intersected\n",
      "6379 intersected\n",
      "6380 intersected\n",
      "6381 intersected\n",
      "6382 intersected\n",
      "6383 intersected\n",
      "6384 intersected\n",
      "6385 intersected\n",
      "6386 intersected\n",
      "6387 intersected\n",
      "6388 intersected\n",
      "6389 intersected\n",
      "6390 intersected\n",
      "6391 intersected\n",
      "6392 intersected\n",
      "6393 intersected\n",
      "6394 intersected\n",
      "6395 intersected\n",
      "6396 intersected\n",
      "6397 intersected\n",
      "6398 intersected\n",
      "6399 intersected\n",
      "6400 intersected\n",
      "6401 intersected\n",
      "6402 intersected\n",
      "6403 intersected\n",
      "6404 intersected\n",
      "6405 intersected\n",
      "6406 intersected\n",
      "6407 intersected\n",
      "6408 intersected\n",
      "6409 intersected\n",
      "6410 intersected\n",
      "6411 intersected\n",
      "6412 intersected\n",
      "6413 intersected\n",
      "6414 intersected\n",
      "6415 intersected\n",
      "6416 intersected\n",
      "6417 intersected\n",
      "6418 intersected\n",
      "6419 intersected\n",
      "6420 intersected\n",
      "6421 intersected\n",
      "6422 intersected\n",
      "6423 intersected\n",
      "6424 intersected\n",
      "6425 intersected\n",
      "6426 intersected\n",
      "6427 intersected\n",
      "6428 intersected\n",
      "6429 intersected\n",
      "6430 intersected\n",
      "6431 intersected\n",
      "6432 intersected\n",
      "6433 intersected\n",
      "6434 intersected\n",
      "6435 intersected\n",
      "6436 intersected\n",
      "6437 intersected\n",
      "6438 intersected\n",
      "6439 intersected\n",
      "6440 intersected\n",
      "6441 intersected\n",
      "6442 intersected\n",
      "6443 intersected\n",
      "6444 intersected\n",
      "6445 intersected\n",
      "6446 intersected\n",
      "6447 intersected\n",
      "6448 intersected\n",
      "6449 intersected\n",
      "6450 intersected\n",
      "6451 intersected\n",
      "6452 intersected\n",
      "6453 intersected\n",
      "6454 intersected\n",
      "6455 intersected\n",
      "6456 intersected\n",
      "6457 intersected\n",
      "6458 intersected\n",
      "6459 intersected\n",
      "6460 intersected\n",
      "6461 intersected\n",
      "6462 intersected\n",
      "6463 intersected\n",
      "6464 intersected\n",
      "6465 intersected\n",
      "6466 intersected\n",
      "6467 intersected\n",
      "6468 intersected\n",
      "6469 intersected\n",
      "6470 intersected\n",
      "6471 intersected\n",
      "6472 intersected\n",
      "6473 intersected\n",
      "6474 intersected\n",
      "6475 intersected\n",
      "6476 intersected\n",
      "6477 intersected\n",
      "6478 intersected\n",
      "6479 intersected\n",
      "6480 intersected\n",
      "6481 intersected\n",
      "6482 intersected\n",
      "6483 intersected\n",
      "6484 intersected\n",
      "6485 intersected\n",
      "6486 intersected\n",
      "6487 intersected\n",
      "6488 intersected\n",
      "6489 intersected\n",
      "6490 intersected\n",
      "6491 intersected\n",
      "6492 intersected\n",
      "6493 intersected\n",
      "6494 intersected\n",
      "6495 intersected\n",
      "6496 intersected\n",
      "6497 intersected\n",
      "6498 intersected\n",
      "6499 intersected\n",
      "6500 intersected\n",
      "6501 intersected\n",
      "6502 intersected\n",
      "6503 intersected\n",
      "6504 intersected\n",
      "6505 intersected\n",
      "6506 intersected\n",
      "6507 intersected\n",
      "6508 intersected\n",
      "6509 intersected\n",
      "6510 intersected\n",
      "6511 intersected\n",
      "6512 intersected\n",
      "6513 intersected\n",
      "6514 intersected\n",
      "6515 intersected\n",
      "6516 intersected\n",
      "6517 intersected\n",
      "6518 intersected\n",
      "6519 intersected\n",
      "6520 intersected\n",
      "6521 intersected\n",
      "6522 intersected\n",
      "6523 intersected\n",
      "6524 intersected\n",
      "6525 intersected\n",
      "6526 intersected\n",
      "6527 intersected\n",
      "6528 intersected\n",
      "6529 intersected\n",
      "6530 intersected\n",
      "6531 intersected\n",
      "6532 intersected\n",
      "6533 intersected\n",
      "6534 intersected\n",
      "6535 intersected\n",
      "6536 intersected\n",
      "6537 intersected\n",
      "6538 intersected\n",
      "6539 intersected\n",
      "6540 intersected\n",
      "6541 intersected\n",
      "6542 intersected\n",
      "6543 intersected\n",
      "6544 intersected\n",
      "6545 intersected\n",
      "6546 intersected\n",
      "6547 intersected\n",
      "6548 intersected\n",
      "6549 intersected\n",
      "6550 intersected\n",
      "6551 intersected\n",
      "6552 intersected\n",
      "6553 intersected\n",
      "6554 intersected\n",
      "6555 intersected\n",
      "6556 intersected\n",
      "6557 intersected\n",
      "6558 intersected\n",
      "6559 intersected\n",
      "6560 intersected\n",
      "6561 intersected\n",
      "6562 intersected\n",
      "6563 intersected\n",
      "6564 intersected\n",
      "6565 intersected\n",
      "6566 intersected\n",
      "6567 intersected\n",
      "6568 intersected\n",
      "6569 intersected\n",
      "6570 intersected\n",
      "6571 intersected\n",
      "6572 intersected\n",
      "6573 intersected\n",
      "6574 intersected\n",
      "6575 intersected\n",
      "6576 intersected\n",
      "6577 intersected\n",
      "6578 intersected\n",
      "6579 intersected\n",
      "6580 intersected\n",
      "6581 intersected\n",
      "6582 intersected\n",
      "6583 intersected\n",
      "6584 intersected\n",
      "6585 intersected\n",
      "6586 intersected\n",
      "6587 intersected\n",
      "6588 intersected\n",
      "6589 intersected\n",
      "6590 intersected\n",
      "6591 intersected\n",
      "6592 intersected\n",
      "6593 intersected\n",
      "6594 intersected\n",
      "6595 intersected\n",
      "6596 intersected\n",
      "6597 intersected\n",
      "6598 intersected\n",
      "6599 intersected\n",
      "6600 intersected\n",
      "6601 intersected\n",
      "6602 intersected\n",
      "6603 intersected\n",
      "6604 intersected\n",
      "6605 intersected\n",
      "6606 intersected\n",
      "6607 intersected\n",
      "6608 intersected\n",
      "6609 intersected\n",
      "6610 intersected\n",
      "6611 intersected\n",
      "6612 intersected\n",
      "6613 intersected\n",
      "6614 intersected\n",
      "6615 intersected\n",
      "6616 intersected\n",
      "6617 intersected\n",
      "6618 intersected\n",
      "6619 intersected\n",
      "6620 intersected\n",
      "6621 intersected\n",
      "6622 intersected\n",
      "6623 intersected\n",
      "6624 intersected\n",
      "6625 intersected\n",
      "6626 intersected\n",
      "6627 intersected\n",
      "6628 intersected\n",
      "6629 intersected\n",
      "6630 intersected\n",
      "6631 intersected\n",
      "6632 intersected\n",
      "6633 intersected\n",
      "6634 intersected\n",
      "6635 intersected\n",
      "6636 intersected\n",
      "6637 intersected\n",
      "6638 intersected\n",
      "6639 intersected\n",
      "6640 intersected\n",
      "6641 intersected\n",
      "6642 intersected\n",
      "6643 intersected\n",
      "6644 intersected\n",
      "6645 intersected\n",
      "6646 intersected\n",
      "6647 intersected\n",
      "6648 intersected\n",
      "6649 intersected\n",
      "6650 intersected\n",
      "6651 intersected\n",
      "6652 intersected\n",
      "6653 intersected\n",
      "6654 intersected\n",
      "6655 intersected\n",
      "6656 intersected\n",
      "6657 intersected\n",
      "6658 intersected\n",
      "6659 intersected\n",
      "6660 intersected\n",
      "6661 intersected\n",
      "6662 intersected\n",
      "6663 intersected\n",
      "6664 intersected\n",
      "6665 intersected\n",
      "6666 intersected\n",
      "6667 intersected\n",
      "6668 intersected\n",
      "6669 intersected\n",
      "6670 intersected\n",
      "6671 intersected\n",
      "6672 intersected\n",
      "6673 intersected\n",
      "6674 intersected\n",
      "6675 intersected\n",
      "6676 intersected\n",
      "6677 intersected\n",
      "6678 intersected\n",
      "6679 intersected\n",
      "6680 intersected\n",
      "6681 intersected\n",
      "6682 intersected\n",
      "6683 intersected\n",
      "6684 intersected\n",
      "6685 intersected\n",
      "6686 intersected\n",
      "6687 intersected\n",
      "6688 intersected\n",
      "6689 intersected\n",
      "6690 intersected\n",
      "6691 intersected\n",
      "6692 intersected\n",
      "6693 intersected\n",
      "6694 intersected\n",
      "6695 intersected\n",
      "6696 intersected\n",
      "6697 intersected\n",
      "6698 intersected\n",
      "6699 intersected\n",
      "6700 intersected\n",
      "6701 intersected\n",
      "6702 intersected\n",
      "6703 intersected\n",
      "6704 intersected\n",
      "6705 intersected\n",
      "6706 intersected\n",
      "6707 intersected\n",
      "6708 intersected\n",
      "6709 intersected\n",
      "6710 intersected\n",
      "6711 intersected\n",
      "6712 intersected\n",
      "6713 intersected\n",
      "6714 intersected\n",
      "6715 intersected\n",
      "6716 intersected\n",
      "6717 intersected\n",
      "6718 intersected\n",
      "6719 intersected\n",
      "6720 intersected\n",
      "6721 intersected\n",
      "6722 intersected\n",
      "6723 intersected\n",
      "6724 intersected\n",
      "6725 intersected\n",
      "6726 intersected\n",
      "6727 intersected\n",
      "6728 intersected\n",
      "6729 intersected\n",
      "6730 intersected\n",
      "6731 intersected\n",
      "6732 intersected\n",
      "6733 intersected\n",
      "6734 intersected\n",
      "6735 intersected\n",
      "6736 intersected\n",
      "6737 intersected\n",
      "6738 intersected\n",
      "6739 intersected\n",
      "6740 intersected\n",
      "6741 intersected\n",
      "6742 intersected\n",
      "6743 intersected\n",
      "6744 intersected\n",
      "6745 intersected\n",
      "6746 intersected\n",
      "6747 intersected\n",
      "6748 intersected\n",
      "6749 intersected\n",
      "6750 intersected\n",
      "6751 intersected\n",
      "6752 intersected\n",
      "6753 intersected\n",
      "6754 intersected\n",
      "6755 intersected\n",
      "6756 intersected\n",
      "6757 intersected\n",
      "6758 intersected\n",
      "6759 intersected\n",
      "6760 intersected\n",
      "6761 intersected\n",
      "6762 intersected\n",
      "6763 intersected\n",
      "6764 intersected\n",
      "6765 intersected\n",
      "6766 intersected\n",
      "6767 intersected\n",
      "6768 intersected\n",
      "6769 intersected\n",
      "6770 intersected\n",
      "6771 intersected\n",
      "6772 intersected\n",
      "6773 intersected\n",
      "6774 intersected\n",
      "6775 intersected\n",
      "6776 intersected\n",
      "6777 intersected\n",
      "6778 intersected\n",
      "6779 intersected\n",
      "6780 intersected\n",
      "6781 intersected\n",
      "6782 intersected\n",
      "6783 intersected\n",
      "6784 intersected\n",
      "6785 intersected\n",
      "6786 intersected\n",
      "6787 intersected\n",
      "6788 intersected\n",
      "6789 intersected\n",
      "6790 intersected\n",
      "6791 intersected\n",
      "6792 intersected\n",
      "6793 intersected\n",
      "6794 intersected\n",
      "6795 intersected\n",
      "6796 intersected\n",
      "6797 intersected\n",
      "6798 intersected\n",
      "6799 intersected\n",
      "6800 intersected\n",
      "6801 intersected\n",
      "6802 intersected\n",
      "6803 intersected\n",
      "6804 intersected\n",
      "6805 intersected\n",
      "6806 intersected\n",
      "6807 intersected\n",
      "6808 intersected\n",
      "6809 intersected\n",
      "6810 intersected\n",
      "6811 intersected\n",
      "6812 intersected\n",
      "6813 intersected\n",
      "6814 intersected\n",
      "6815 intersected\n",
      "6816 intersected\n",
      "6817 intersected\n",
      "6818 intersected\n",
      "6819 intersected\n",
      "6820 intersected\n",
      "6821 intersected\n",
      "6822 intersected\n",
      "6823 intersected\n",
      "6824 intersected\n",
      "6825 intersected\n",
      "6826 intersected\n",
      "6827 intersected\n",
      "6828 intersected\n",
      "6829 intersected\n",
      "6830 intersected\n",
      "6831 intersected\n",
      "6832 intersected\n",
      "6833 intersected\n",
      "6834 intersected\n",
      "6835 intersected\n",
      "Intersections have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Calculate intersections\n",
    "# Load transects \n",
    "try:\n",
    "    transects = gpd.read_file(transects_path)\n",
    "    print(\"Transects have been loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Transects file does not exist.\")\n",
    "\n",
    "# Intersections\n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "intersections_file = os.path.join(data_dir,tile_name,tile_name+\"_intersections\")\n",
    "if not os.path.exists(intersections_file):\n",
    "    try:\n",
    "        shorelines = gpd.read_file(os.path.join(data_dir,tile_name,tile_name+\"_shorelines\"))\n",
    "    except FileNotFoundError:\n",
    "        print('Shorelines do not exist.')\n",
    "    else:        \n",
    "        print(\"Calcualte intersections...\")\n",
    "        #tile_poly = tile.geometry\n",
    "        #transects = gpd.clip(transects,tile_poly)\n",
    "        intersections = postprocess.compute_intersections(transects,shorelines,remove_outliers=False)\n",
    "        if not intersections.empty:\n",
    "            intersections.to_file(intersections_file,driver=\"GPKG\")\n",
    "            print(\"Intersections have been created and saved.\")\n",
    "        else:\n",
    "            print(\"No intersections available for\",tile_name)\n",
    "else:\n",
    "    print(\"Intersections already exist.\")"
   ]
  },
  {
   "source": [
    "### 8.| Coastline Change Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Hotspot analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transect 3117 removed.\n",
      "Transect 3121 removed.\n",
      "Transect 3122 removed.\n",
      "Transect 3184 removed.\n",
      "Transect 3185 removed.\n",
      "Transect 3186 removed.\n",
      "Transect 3204 removed.\n",
      "Transect 3233 removed.\n",
      "Transect 3259 removed.\n",
      "Transect 3283 removed.\n",
      "Transect 3284 removed.\n",
      "Transect 3285 removed.\n",
      "Transect 3309 removed.\n",
      "Transect 3310 removed.\n",
      "Transect 3311 removed.\n",
      "Transect 3312 removed.\n",
      "Transect 3313 removed.\n",
      "Transect 3314 removed.\n",
      "Transect 3315 removed.\n",
      "Transect 3316 removed.\n",
      "Transect 3341 removed.\n",
      "Transect 3354 removed.\n",
      "Transect 3355 removed.\n",
      "Transect 3357 removed.\n",
      "Transect 3358 removed.\n",
      "Transect 3359 removed.\n",
      "Transect 3360 removed.\n",
      "Transect 3361 removed.\n",
      "Transect 3362 removed.\n",
      "Transect 3363 removed.\n",
      "Transect 3374 removed.\n",
      "Transect 3430 removed.\n",
      "Transect 3431 removed.\n",
      "Transect 3432 removed.\n",
      "Transect 3433 removed.\n",
      "Transect 3434 removed.\n",
      "Transect 3511 removed.\n",
      "Transect 3512 removed.\n",
      "Transect 3513 removed.\n",
      "Transect 3517 removed.\n",
      "Transect 3762 removed.\n",
      "Transect 3763 removed.\n",
      "Transect 3764 removed.\n",
      "Transect 3765 removed.\n",
      "Transect 3766 removed.\n",
      "Transect 3767 removed.\n",
      "Transect 3768 removed.\n",
      "Transect 3769 removed.\n",
      "Transect 3770 removed.\n",
      "Transect 3771 removed.\n",
      "Transect 3772 removed.\n",
      "Transect 3782 removed.\n",
      "Transect 3783 removed.\n",
      "Transect 3788 removed.\n",
      "Transect 3789 removed.\n",
      "Transect 3790 removed.\n",
      "Transect 3791 removed.\n",
      "Transect 3792 removed.\n",
      "Transect 3794 removed.\n",
      "Transect 3795 removed.\n",
      "Transect 3800 removed.\n",
      "Transect 3801 removed.\n",
      "Transect 3802 removed.\n",
      "Transect 3803 removed.\n",
      "Transect 3804 removed.\n",
      "Transect 3805 removed.\n",
      "Transect 3806 removed.\n",
      "Transect 3807 removed.\n",
      "Transect 3808 removed.\n",
      "Transect 3841 removed.\n",
      "Transect 3842 removed.\n",
      "Transect 3843 removed.\n",
      "Transect 3844 removed.\n",
      "Transect 3845 removed.\n",
      "Transect 3929 removed.\n",
      "Transect 3930 removed.\n",
      "Transect 3931 removed.\n",
      "Transect 3932 removed.\n",
      "Transect 3933 removed.\n",
      "Transect 3934 removed.\n",
      "Transect 3935 removed.\n",
      "Transect 3936 removed.\n",
      "Transect 3937 removed.\n",
      "Transect 3945 removed.\n",
      "Classifications saved.\n"
     ]
    }
   ],
   "source": [
    "# Create classification transects (Erosion, Accretion, Stable, etc.)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "classifications_path = os.path.join(data_dir,\"AKn_all_classifications\")\n",
    "if not os.path.exists(classifications_path):\n",
    "    intersections = gpd.read_file(os.path.join(data_dir,tile_name,tile_name+\"_intersections\"))\n",
    "    classifications = postprocess.calc_change_metrics(intersections,2)\n",
    "    classifications.to_file(classifications_path,driver=\"GPKG\")\n",
    "    print(\"Classifications saved.\")\n",
    "else:\n",
    "    print(\"Classifications exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All hotspots identified.\n",
      "Hotspot files saved.\n"
     ]
    }
   ],
   "source": [
    "### THIS HAS NOT BEEN DONE FOR ALASKA YET. \n",
    "\n",
    "# Calculate accretion and erosion hotspots \n",
    "erosion_hotspots_path = os.path.join(data_dir,\"VN_erosion_hotspots\")\n",
    "accretion_hotspots_path = os.path.join(data_dir,\"VN_accretion_hotspots\")\n",
    "\n",
    "if not os.path.exists(erosion_hotspots_path+\"a\"):\n",
    "    # load an prepare classification file \n",
    "    classification = gpd.read_file(os.path.join(data_dir,\"VN_all_classifications\"))\n",
    "    classification = classification.sort_values(by=\"Transect_id\").reset_index(drop=True)\n",
    "    #classification = classification.replace(\"nan\",np.NaN)\n",
    "    classification = classification[classification[\"class_L1\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # set up while loop to find clustered erosion and accretion classification transects\n",
    "    yet_seen = []\n",
    "    erosion_hotspots = []\n",
    "    accretion_hotspots = []\n",
    "    # iterate through all classification transects\n",
    "    for t, transect in classification.iterrows():\n",
    "            # check if transect has already been evaluated \n",
    "            if not t in yet_seen:\n",
    "                cluster = []\n",
    "                # create cluster if the next transect or the transect after has the same class \n",
    "                while (transect.class_L1 == classification.iloc[t].class_L1) or (transect.class_L1 == classification.iloc[t+1].class_L1) :\n",
    "                    yet_seen.append(t)\n",
    "                    cluster.append(classification.iloc[t])\n",
    "                    t +=1\n",
    "                    # exist while loop if last transect id +2 has been reached\n",
    "                    if t+2 > len(classification):\n",
    "                        break\n",
    "                # if less than 20 transect share the same class, omit from hotspot analysis\n",
    "                if len(cluster)>20:\n",
    "                    # save only erosion and accretion clusters and convert to GeoDataFrame\n",
    "                    if cluster[0].class_L1 == \"Accretion\":\n",
    "                        gdf = gpd.GeoDataFrame(cluster)\n",
    "                        accretion_hotspots.append(gdf)\n",
    "                    elif cluster[0].class_L1 == \"Erosion\":\n",
    "                        gdf = gpd.GeoDataFrame(cluster)\n",
    "                        erosion_hotspots.append(gdf)\n",
    "    print(\"All hotspots identified.\")\n",
    "    # add cluster number to hotspot transects\n",
    "    for i, transect in enumerate(erosion_hotspots):\n",
    "        transect['cluster_no'] = i\n",
    "    for i, transect in enumerate(accretion_hotspots):\n",
    "        transect['cluster_no'] = i \n",
    "    # merge all hotspot dataframes\n",
    "    erosion_hotspots_gdf = pd.concat(erosion_hotspots)\n",
    "    accretion_hotspots_gdf = pd.concat(accretion_hotspots)\n",
    "    # save dataframes\n",
    "    erosion_hotspots_gdf.to_file(erosion_hotspots_path,driver=\"GPKG\")\n",
    "    accretion_hotspots_gdf.to_file(accretion_hotspots_path,driver=\"GPKG\")\n",
    "    print(\"Hotspot files saved.\")\n",
    "else:\n",
    "    print(\"Hotspot files exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extreme hotspot files exist.\n"
     ]
    }
   ],
   "source": [
    "# Filter extreme erosion and accretion hotspots\n",
    "extreme_erosion_hotspots_path = os.path.join(data_dir,\"VN_extreme_erosion_hotspots\")\n",
    "extreme_accretion_hotspots_path = os.path.join(data_dir,\"VN_extreme_accretion_hotspots\")\n",
    "if not os.path.exists(extreme_erosion_hotspots_path):\n",
    "    erosion_hotspots = gpd.read_file(os.path.join(data_dir,\"VN_erosion_hotspots\"))\n",
    "    accretion_hotspots = gpd.read_file(os.path.join(data_dir,\"VN_accretion_hotspots\"))\n",
    "    print(\"Mean erosion of extreme hotspots:\")\n",
    "    extreme_erosion_hotspots = postprocess.define_severe_hotspots(erosion_hotspots,-5,\"smaller\")\n",
    "    print(\"Mean accretion of extreme hotspots:\")\n",
    "    extreme_accretion_hotspots = postprocess.define_severe_hotspots(accretion_hotspots,5,\"bigger\")\n",
    "    # save files\n",
    "    extreme_erosion_hotspots.to_file(extreme_erosion_hotspots_path,driver=\"GPKG\")\n",
    "    extreme_accretion_hotspots.to_file(extreme_accretion_hotspots_path,driver=\"GPKG\")\n",
    "else:\n",
    "    print(\"Extreme hotspot files exist.\")"
   ]
  },
  {
   "source": [
    "#### Land area change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coastal provinces are already clipped to buffer and read.\nLand area change in province:  VN717\nLand area file already exists.\nLand area change in province:  VN821\nLand area file already exists.\nLand area change in province:  VN811\nLand area file already exists.\nLand area change in province:  VN507\nLand area file already exists.\nLand area change in province:  VN715\nLand area file already exists.\nLand area change in province:  VN823\nLand area file already exists.\nLand area change in province:  VN501\nLand area file already exists.\nLand area change in province:  VN713\nLand area file already exists.\nLand area change in province:  VN405\nLand area file already exists.\nLand area change in province:  VN103\nLand area file already exists.\nLand area change in province:  VN701\nLand area file already exists.\nLand area change in province:  VN511\nLand area file already exists.\nLand area change in province:  VN813\nLand area file already exists.\nLand area change in province:  VN113\nLand area file already exists.\nLand area change in province:  VN403\nLand area file already exists.\nLand area change in province:  VN117\nLand area file already exists.\nLand area change in province:  VN705\nLand area file already exists.\nLand area change in province:  VN509\nLand area file already exists.\nLand area change in province:  VN407\nLand area file already exists.\nLand area change in province:  VN503\nLand area file already exists.\nLand area change in province:  VN505\nLand area file already exists.\nLand area change in province:  VN225\nLand area file already exists.\nLand area change in province:  VN409\nLand area file already exists.\nLand area change in province:  VN819\nLand area file already exists.\nLand area change in province:  VN115\nLand area file already exists.\nLand area change in province:  VN401\nLand area file already exists.\nLand area change in province:  VN411\nLand area file already exists.\nLand area change in province:  VN807\nLand area file already exists.\nLand area change in province:  VN817\nLand area file already exists.\n"
     ]
    }
   ],
   "source": [
    "# calcualte land area change in the coastal zone (5km buffer) as proportional change \n",
    "provinces_clip_path = os.path.join(data_dir,\"VN_coastal_provinces_clip\")\n",
    "provinces_path = os.path.join(data_dir,\"VN_coastal_provinces\")\n",
    "buffer_path = os.path.join(data_dir,\"VN_buffer_5km\")\n",
    "\n",
    "# buffer coastal provinces by 5km and clip to buffer \n",
    "if not os.path.exists(provinces_clip_path):\n",
    "    buffer = gpd.read_file(buffer_path)\n",
    "    provinces = gpd.read_file(provinces_path)\n",
    "    provinces = provinces.to_crs(crs)\n",
    "    provinces['geometry'] = provinces.geometry.buffer(5000)\n",
    "    provinces = gpd.clip(provinces,buffer)\n",
    "    provinces.to_file(provinces_clip_path,driver=\"GeoJSON\")\n",
    "else:\n",
    "    print(\"Coastal provinces are already clipped to buffer and read.\")\n",
    "    provinces = gpd.read_file(provinces_clip_path)\n",
    "\n",
    "# calcualte proportional land area change for each province \n",
    "for i in provinces.index:\n",
    "    name = provinces.ADM1_PCODE.iloc[i]\n",
    "    print(\"Land area change in province: \",name)\n",
    "    land_area_path = os.path.join(plot_dir,\"land_area_change_\"+name)\n",
    "    if not os.path.exists(land_area_path):\n",
    "        land_area = pd.DataFrame(columns=['year','land_area_percentage'])\n",
    "        land_area['year'] = range(1987,2022)\n",
    "        for year in range(1987,2022):\n",
    "            print(year)\n",
    "            binary_files = glob.glob(os.path.join(\"{path}/**/*{year}*bin.tif\".format(path=data_dir,year=year)))\n",
    "            binary_files.sort()\n",
    "            land_pixels, valid_pixels = [],[]\n",
    "            for file in binary_files:\n",
    "                # crop raster to area\n",
    "                area = provinces[provinces.index==i]\n",
    "                with rio.open(file) as src:\n",
    "                    try:\n",
    "                        out_image, out_transform = rio.mask.mask(src,area.geometry,crop=True,nodata=np.nan)\n",
    "                        out_meta = src.meta\n",
    "                        out_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": out_image.shape[1],\n",
    "                            \"width\": out_image.shape[2],\n",
    "                            \"transform\": out_transform,\n",
    "                            \"compress\":\"LZW\"})\n",
    "                        out_path = os.path.join(os.getcwd(),\"test_data\",str(year)+\"_\"+name+\"_\"+os.path.basename(file)[:3])\n",
    "                        with rio.open(out_path,\"w\",**out_meta) as dst:\n",
    "                            dst.write(out_image)\n",
    "                        show(out_image)\n",
    "                        im_rev = out_image.copy()\n",
    "                        im_rev[im_rev==0]=2\n",
    "                        im_rev[im_rev==1]=0\n",
    "                        im_rev[im_rev==2]=1\n",
    "                        valid_pixel = np.count_nonzero(~np.isnan(out_image))\n",
    "                        land_pixel = np.nansum(im_rev)#*30*30\n",
    "                        land_pixels.append(land_pixel)\n",
    "                        valid_pixels.append(valid_pixel)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            land_pixel_percentage = np.sum(land_pixels)/np.sum(valid_pixels)*100\n",
    "            print(\"\\nLand pixel percentage in\",str(year)+\":\", land_pixel_percentage,\"\\n\")\n",
    "            land_area.loc[land_area.year == year, \"land_area_percentage\"] = land_pixel_percentage\n",
    "            land_area.to_csv(land_area_name)\n",
    "    else:\n",
    "        print(\"Land area file already exists.\")"
   ]
  }
 ]
}