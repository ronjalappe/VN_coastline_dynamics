{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('geo_env': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7bf14925a434f6b7c958ecb24d261e8248047be1db188dfff6ca1cc92668254a"
   }
  },
  "interpreter": {
   "hash": "ba7df6755b32bab95c00c96c0cebd9d890b50759e401e4d74df893f436e34bc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Shoreline extraction from Landsat MNDWI images for Sylt, Northern Germany (1986-2021)\n",
    "\n",
    "This notebook extracts subpixel contours from MNDWI images, which have been processing on  Google Earth Engine (see: https://code.earthengine.google.com/9d450b3d0d9840a352cbd3ba6ddb6ca5) and quantifies coastline change along shore-perpendicular transects.\n",
    "\n",
    "Content of the notebook:\n",
    "\n",
    "1. Setup\n",
    "2. Download MNDWI rasters from Google Drive\n",
    "3. Reproject and quality check MNDWI rasters\n",
    "4. Subpixel contours\n",
    "5. Create minimum water extent polygon\n",
    "6. Create transects\n",
    "7. Calculate intersections between shorelines and transects\n",
    "8. Coastline Change Analysis\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.| Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import glob \n",
    "import numpy as np \n",
    "import shapely as shp\n",
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import rasterio as rio \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats \n",
    "from skimage.filters import threshold_otsu\n",
    "# my coastline methods \n",
    "from coasty import postprocess, analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "data_dir = os.path.join(os.getcwd(),\"data/Sylt\") # path to data-folder with aux data\n",
    "plot_dir = os.path.join(os.getcwd(),\"figures/plots\")\n",
    "country_bounds_path = os.path.join(data_dir,\"Sylt_GDAM_bounds.geojson\") # path to country bounds\n",
    "transects_path = os.path.join(data_dir,\"Sylt_transects_gov_2km_200m\")\n",
    "osm_sl_path = os.path.join(os.path.join(data_dir,\"Sylt_osm_coastline.geojson\")) # path to reference shoreline\n",
    "buffer_path = os.path.join(os.path.join(data_dir,\"Sylt_buffer_5km\"))\n",
    "tile_name = \"Sylt\"\n",
    "\n",
    "# Params\n",
    "export_folder = \"GEE\"       # folder on Google Drive with GEE images to download\n",
    "crs = \"EPSG:23031\" # UTM ZONE 31N   # coordinate system code of a projected crs \n",
    "min_length = 3000                   # min length of shoreline to keep [m]\n",
    "buffer_dist = 5000                  # buffer around reference shorelines to clip detected shorelines [m]\n",
    "transect_len = 1000                 # length of transects [m]\n",
    "transect_dist = 200                 # distance between transects [m]\n",
    "transect_min_line_length = 10000    # min legnth of polygon outline at which to draw transects [m]\n",
    "                                    # (for removing small islands) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.972972972972974\n3\n21\n"
     ]
    }
   ],
   "source": [
    "shorelines = gpd.read_file(os.path.join(data_dir,tile_name,tile_name+\"_shorelines\"))\n",
    "print(np.mean(shorelines.avg_aq.astype(int)))\n",
    "print(np.min(shorelines.avg_aq.astype(int)))\n",
    "print(np.max(shorelines.avg_aq.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Everything successfully read.\n"
     ]
    }
   ],
   "source": [
    "# read/ create aux data \n",
    "#country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "osm_sl = gpd.read_file(osm_sl_path).to_crs(crs)\n",
    "\n",
    "try:\n",
    "    buffer = gpd.read_file(buffer_path)\n",
    "    print(\"Everything successfully read.\")\n",
    "except:\n",
    "    print(\"Create osm shoreline buffer:\")\n",
    "    buffer = osm_sl.buffer(buffer_dist)\n",
    "    buffer.to_file(buffer_path,driver=\"GeoJSON\")\n",
    "    print(\"Buffer saved.\")"
   ]
  },
  {
   "source": [
    "### 2.| Download MNDWI rasters from Google Drive\n",
    "done manually in this case. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.| Reproject and quality check MNDWI rasters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Sylt --------------------\n",
      "Sylt_1984_04avg_aq.tif already projected to given CRS.\n",
      "Sylt_1984_04avg_aq.tif could not be masked\n",
      "Sylt_1985_05avg_aq.tif already projected to given CRS.\n",
      "Sylt_1985_05avg_aq.tif could not be masked\n",
      "Sylt_1986_04avg_aq.tif already projected to given CRS.\n",
      "Sylt_1986_04avg_aq.tif could not be masked\n",
      "Sylt_1987_06avg_aq.tif already projected to given CRS.\n",
      "Sylt_1987_06avg_aq.tif could not be masked\n",
      "Sylt_1988_03avg_aq.tif already projected to given CRS.\n",
      "Sylt_1988_03avg_aq.tif could not be masked\n",
      "Sylt_1989_08avg_aq.tif already projected to given CRS.\n",
      "Sylt_1989_08avg_aq.tif could not be masked\n",
      "Sylt_1990_09avg_aq.tif already projected to given CRS.\n",
      "Sylt_1990_09avg_aq.tif could not be masked\n",
      "Sylt_1991_03avg_aq.tif already projected to given CRS.\n",
      "Sylt_1991_03avg_aq.tif could not be masked\n",
      "Sylt_1992_06avg_aq.tif already projected to given CRS.\n",
      "Sylt_1992_06avg_aq.tif could not be masked\n",
      "Sylt_1993_04avg_aq.tif already projected to given CRS.\n",
      "Sylt_1993_04avg_aq.tif could not be masked\n",
      "Sylt_1994_08avg_aq.tif already projected to given CRS.\n",
      "Sylt_1994_08avg_aq.tif could not be masked\n",
      "Sylt_1995_10avg_aq.tif already projected to given CRS.\n",
      "Sylt_1995_10avg_aq.tif could not be masked\n",
      "Sylt_1996_04avg_aq.tif already projected to given CRS.\n",
      "Sylt_1996_04avg_aq.tif could not be masked\n",
      "Sylt_1997_06avg_aq.tif already projected to given CRS.\n",
      "Sylt_1997_06avg_aq.tif could not be masked\n",
      "Sylt_1998_05avg_aq.tif already projected to given CRS.\n",
      "Sylt_1998_05avg_aq.tif could not be masked\n",
      "Sylt_1999_13avg_aq.tif already projected to given CRS.\n",
      "Sylt_1999_13avg_aq.tif could not be masked\n",
      "Sylt_2000_16avg_aq.tif already projected to given CRS.\n",
      "Sylt_2000_16avg_aq.tif could not be masked\n",
      "Sylt_2001_14avg_aq.tif already projected to given CRS.\n",
      "Sylt_2001_14avg_aq.tif could not be masked\n",
      "Sylt_2002_15avg_aq.tif already projected to given CRS.\n",
      "Sylt_2002_15avg_aq.tif could not be masked\n",
      "Sylt_2003_15avg_aq.tif already projected to given CRS.\n",
      "Sylt_2003_15avg_aq.tif could not be masked\n",
      "Sylt_2004_16avg_aq.tif already projected to given CRS.\n",
      "Sylt_2004_16avg_aq.tif could not be masked\n",
      "Sylt_2005_13avg_aq.tif already projected to given CRS.\n",
      "Sylt_2005_13avg_aq.tif could not be masked\n",
      "Sylt_2006_11avg_aq.tif already projected to given CRS.\n",
      "Sylt_2006_11avg_aq.tif could not be masked\n",
      "Sylt_2007_09avg_aq.tif already projected to given CRS.\n",
      "Sylt_2007_09avg_aq.tif could not be masked\n",
      "Sylt_2008_09avg_aq.tif already projected to given CRS.\n",
      "Sylt_2008_09avg_aq.tif could not be masked\n",
      "Sylt_2009_12avg_aq.tif already projected to given CRS.\n",
      "Sylt_2009_12avg_aq.tif could not be masked\n",
      "Sylt_2010_13avg_aq.tif already projected to given CRS.\n",
      "Sylt_2010_13avg_aq.tif could not be masked\n",
      "Sylt_2011_12avg_aq.tif already projected to given CRS.\n",
      "Sylt_2011_12avg_aq.tif could not be masked\n",
      "Sylt_2012_06avg_aq.tif already projected to given CRS.\n",
      "Sylt_2012_06avg_aq.tif could not be masked\n",
      "Sylt_2013_13avg_aq.tif already projected to given CRS.\n",
      "Sylt_2013_13avg_aq.tif could not be masked\n",
      "Sylt_2014_21avg_aq.tif already projected to given CRS.\n",
      "Sylt_2014_21avg_aq.tif could not be masked\n",
      "Sylt_2015_15avg_aq.tif already projected to given CRS.\n",
      "Sylt_2015_15avg_aq.tif could not be masked\n",
      "Sylt_2016_19avg_aq.tif already projected to given CRS.\n",
      "Sylt_2016_19avg_aq.tif could not be masked\n",
      "Sylt_2017_17avg_aq.tif already projected to given CRS.\n",
      "Sylt_2017_17avg_aq.tif could not be masked\n",
      "Sylt_2018_21avg_aq.tif already projected to given CRS.\n",
      "Sylt_2018_21avg_aq.tif could not be masked\n",
      "Sylt_2019_21avg_aq.tif already projected to given CRS.\n",
      "Sylt_2019_21avg_aq.tif could not be masked\n",
      "Sylt_2020_20avg_aq.tif already projected to given CRS.\n",
      "Sylt_2020_20avg_aq.tif could not be masked\n"
     ]
    }
   ],
   "source": [
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "raster_paths = glob.glob(os.path.join(data_dir,tile_name,tile_name+\"_*.tif\"))\n",
    "raster_paths.sort()\n",
    "for r in raster_paths:\n",
    "    masked_file = glob.glob(r[:-4]+\"*aq.tif\") \n",
    "    if len(masked_file) == 0:\n",
    "        postprocess.reproject_raster(r,r,crs)\n",
    "        try:\n",
    "            postprocess.mask_single_observation_pixel(r)\n",
    "            os.remove(r)\n",
    "            print(os.path.basename(r),\"removed.\")\n",
    "        except:\n",
    "            print(os.path.basename(r),'could not be masked')\n",
    "            pass\n",
    "    else:\n",
    "        print(os.path.basename(masked_file[0]), \"already exists.\")"
   ]
  },
  {
   "source": [
    "### 4.| Subpixel contours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Sylt --------------------\n",
      "Process shorelines...\n",
      "0.269078\n",
      "1984: shoreline processed.\n",
      "0.21554148\n",
      "1985: shoreline processed.\n",
      "0.45619684\n",
      "1986: shoreline processed.\n",
      "0.29555875\n",
      "1987: shoreline processed.\n",
      "0.28837383\n",
      "1988: shoreline processed.\n",
      "0.23926374\n",
      "1989: shoreline processed.\n",
      "0.27210644\n",
      "1990: shoreline processed.\n",
      "0.29748198\n",
      "1991: shoreline processed.\n",
      "0.23685223\n",
      "1992: shoreline processed.\n",
      "0.23081285\n",
      "1993: shoreline processed.\n",
      "0.26542163\n",
      "1994: shoreline processed.\n",
      "0.23080173\n",
      "1995: shoreline processed.\n",
      "0.281161\n",
      "1996: shoreline processed.\n",
      "0.2346102\n",
      "1997: shoreline processed.\n",
      "0.22206458\n",
      "1998: shoreline processed.\n",
      "0.2547193\n",
      "1999: shoreline processed.\n",
      "0.252936\n",
      "2000: shoreline processed.\n",
      "0.2610216\n",
      "2001: shoreline processed.\n",
      "0.2614776\n",
      "2002: shoreline processed.\n",
      "0.2408827\n",
      "2003: shoreline processed.\n",
      "0.2651568\n",
      "2004: shoreline processed.\n",
      "0.26486987\n",
      "2005: shoreline processed.\n",
      "0.23793128\n",
      "2006: shoreline processed.\n",
      "0.2672245\n",
      "2007: shoreline processed.\n",
      "0.22464274\n",
      "2008: shoreline processed.\n",
      "0.21339464\n",
      "2009: shoreline processed.\n",
      "0.21248901\n",
      "2010: shoreline processed.\n",
      "0.25725257\n",
      "2011: shoreline processed.\n",
      "0.26822782\n",
      "2012: shoreline processed.\n",
      "0.20911813\n",
      "2013: shoreline processed.\n",
      "0.22757718\n",
      "2014: shoreline processed.\n",
      "0.22566769\n",
      "2015: shoreline processed.\n",
      "0.23123021\n",
      "2016: shoreline processed.\n",
      "0.23738796\n",
      "2017: shoreline processed.\n",
      "0.21741292\n",
      "2018: shoreline processed.\n",
      "0.23389275\n",
      "2019: shoreline processed.\n",
      "0.22656977\n",
      "2020: shoreline processed.\n",
      "All shorelines have been created and saved.\n",
      "CPU times: user 1min 11s, sys: 1.84 s, total: 1min 12s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "# Create shorelines\n",
    "shorelines_path = os.path.join(folder_path,tile_name+\"_shorelines\") \n",
    "if not os.path.exists(shorelines_path):\n",
    "    shorelines = []\n",
    "    print(\"Process shorelines...\")    \n",
    "    raster_paths = glob.glob(os.path.join(data_dir,tile_name,\"*aq.tif\"))\n",
    "    raster_paths.sort()\n",
    "    for r in raster_paths:\n",
    "        # create path for single shorelines \n",
    "        sl_folder_path = os.path.join(folder_path,tile_name+\"_single_shorelines\")\n",
    "        sl_path = os.path.join(sl_folder_path,os.path.splitext(os.path.basename(r))[0]+\"_shoreline\")\n",
    "        # save single shoreline without modifications as backup\n",
    "        if not os.path.exists(sl_path):\n",
    "            with rio.open(r,\"r\") as raster:\n",
    "                mndwi = raster.read(1)\n",
    "                if np.count_nonzero(mndwi) > 0 and np.count_nonzero(~np.isnan(mndwi)) > 0:                    \n",
    "                    thres = threshold_otsu(mndwi[~np.isnan(mndwi)])\n",
    "                    print(thres)\n",
    "                    shoreline = postprocess.subpixel_contours(r,thres)\n",
    "                    if not shoreline.empty:\n",
    "                        if not os.path.exists(sl_folder_path): os.mkdir(sl_folder_path)\n",
    "                        shoreline.to_file(os.path.join(sl_path),driver=\"GeoJSON\")\n",
    "        else:\n",
    "            shoreline = gpd.read_file(sl_path)\n",
    "        # postprocess raw shorelines\n",
    "        shoreline = gpd.clip(shoreline,buffer)\n",
    "        cleaned = postprocess.remove_small_lines(shoreline, min_size=min_length)\n",
    "        if not cleaned.empty:\n",
    "            year = os.path.basename(r)[5:9]\n",
    "            avg_aq = os.path.basename(r)[10:12]\n",
    "            cleaned['id']=year\n",
    "            cleaned = cleaned.dissolve(by=cleaned.id,aggfunc=\"sum\")\n",
    "            cleaned['year']=year\n",
    "            cleaned['avg_aq']=avg_aq\n",
    "            cleaned['otsu_thres']=str(thres)\n",
    "            shorelines.append(cleaned)\n",
    "            print(year+\": shoreline processed.\")\n",
    "    shorelines_gdf = pd.concat(shorelines,ignore_index=True)    \n",
    "    shorelines_gdf.to_file(os.path.join(shorelines_path),driver=\"GPKG\")\n",
    "    print(\"All shorelines have been created and saved.\")\n",
    "else:\n",
    "    print(\"Shorelines already exist.\")"
   ]
  },
  {
   "source": [
    "### 5.| Create minimum water extent polygon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Generate minimum and maximum water extent raster for each processing tile"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Sylt --------------------\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1997_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2011_12avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1994_08avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2016_19avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2006_11avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1986_04avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2007_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1993_04avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2002_15avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1998_05avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2020_20avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1995_10avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2009_12avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2010_13avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2005_13avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2000_16avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2003_15avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2018_21avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1989_08avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1990_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1988_03avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2019_21avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1984_04avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1996_04avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2004_16avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2015_15avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2013_13avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1991_03avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1985_05avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2017_17avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2012_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2001_14avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2014_21avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2008_09avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1987_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1992_06avg_aq_bin.tif saved.\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1999_13avg_aq_bin.tif saved.\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2003_15avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1995_10avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1990_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1992_06avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2010_13avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1991_03avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2019_21avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2020_20avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1989_08avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2002_15avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1984_04avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1987_06avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2012_06avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2009_12avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1998_05avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2005_13avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2001_14avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2018_21avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2006_11avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1997_06avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2011_12avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2015_15avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2008_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2014_21avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1988_03avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1993_04avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2007_09avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2017_17avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1999_13avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1986_04avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1994_08avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2004_16avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2000_16avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1996_04avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2013_13avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_1985_05avg_aq_bin.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/data/Sylt/Sylt/Sylt_2016_19avg_aq_bin.tif\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/coasty/analysis.py:26: RuntimeWarning: All-NaN slice encountered\n",
      "  min_water_extent = np.nanmin(all_masks, 0)  # water = 1, min water extent\n",
      "/Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/coasty/analysis.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "  max_water_extent = np.nanmax(all_masks, 0)  # no water = 0, max water extent\n"
     ]
    }
   ],
   "source": [
    "# Calculate raster with min and max water extent \n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    raster_paths = glob.glob(os.path.join(folder_path,\"*aq.tif\"))\n",
    "    # make MNDWI images binary first\n",
    "    # (this step should later be included to the shoreline extraction script, where the Otsu is already being calculated) \n",
    "    for r in raster_paths:\n",
    "        # create binary raster using the Otsu threshold for min water raster                \n",
    "        binary_file = os.path.join(folder_path,os.path.splitext(os.path.basename(r))[0]+\"_bin.tif\")\n",
    "        if not os.path.exists(binary_file):\n",
    "            with rio.open(r,\"r\") as raster:\n",
    "                mndwi = raster.read(1)\n",
    "                if np.count_nonzero(mndwi) > 0 and np.count_nonzero(~np.isnan(mndwi)) > 0:                    \n",
    "                    meta = raster.meta\n",
    "                    thres = threshold_otsu(mndwi[~np.isnan(mndwi)])\n",
    "                    binary = mndwi.copy()\n",
    "                    binary[binary > thres] = 1\n",
    "                    binary[binary < thres] = 0\n",
    "                    meta.update({\n",
    "                        \"compress\":\"LZW\",\n",
    "                        })\n",
    "                    with rio.open(binary_file,'w',**meta) as dst:\n",
    "                        dst.write(binary,1)\n",
    "                    print(binary_file, \"saved.\")\n",
    "        else:\n",
    "            print(binary_file, \"exists.\")\n",
    "    binary_paths = glob.glob(os.path.join(folder_path,\"*aq_bin.tif\"))\n",
    "    min_water_file = os.path.join(data_dir,tile_name,tile_name+\"_min_water_extent\")\n",
    "    max_water_file = os.path.join(data_dir,tile_name,tile_name+\"_max_water_extent\")\n",
    "    if not os.path.exists(min_water_file):\n",
    "        analysis.calc_water_extent(binary_paths,min_water_file,max_water_file)\n",
    "    else:\n",
    "        print(\"Files exist.\")"
   ]
  },
  {
   "source": [
    "Generalize, vectorize and merge minimum water extent rasters and create transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating Sylt --------------------\n",
      "Pixel cluster removed.\n",
      "Minimum water extent polygon created.\n",
      "\n",
      "CPU times: user 326 ms, sys: 39.1 ms, total: 365 ms\n",
      "Wall time: 448 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#remove small pixel cluster in min and max water extent rasters and merge rasters of all tiles\n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "\n",
    "# define path for generalized min water extent raster \n",
    "min_water_simple_file = os.path.join(data_dir,tile_name,tile_name+\"_min_water_extent_simple\")\n",
    "if not os.path.exists(min_water_simple_file+\"_poly\"):\n",
    "    try:\n",
    "        # read min water extent file\n",
    "        min_water_file = os.path.join(folder_path,tile_name+\"_min_water_extent\")\n",
    "    except FileNotFoundError:\n",
    "        print('File does not exist.')\n",
    "    else:\n",
    "        # remove small objects from raster\n",
    "        analysis.remove_pixel_cluster(min_water_file,min_water_simple_file,50000,100000,0)\n",
    "        print(\"Pixel cluster removed.\")\n",
    "        # vectorize raster \n",
    "        min_water_poly = analysis.vectorize_raster(min_water_simple_file,0)\n",
    "        if not min_water_poly.empty:\n",
    "            min_water_poly.to_file(min_water_simple_file+\"_poly\",driver=\"GeoJSON\")\n",
    "            print(\"Minimum water extent polygon created.\\n\")\n",
    "else:\n",
    "    print(\"Minimum water extent polygon already exists.\\n\")\n"
   ]
  },
  {
   "source": [
    "### 6.| Create transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create transects...\n",
      "(0, 0) n_points: 775\n",
      "Transects have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "#country_bounds.geometry = country_bounds.buffer(-6500)\n",
    "\n",
    "try:\n",
    "    transects = gpd.read_file(transects_path)\n",
    "    print(\"Transects exist and have been loaded.\")\n",
    "except:\n",
    "    print(\"Create transects...\")\n",
    "    country_bounds = country_bounds.explode()\n",
    "    # only take the land polygon to exclude islands etc.\n",
    "    country_bounds['area'] = country_bounds.geometry.area\n",
    "    country_bounds = country_bounds[country_bounds.area == np.max(country_bounds.area)]\n",
    "    #country_bounds_gov.geometry = country_bounds_gov.geometry.simplify(500,preserve_topology=True)\n",
    "    country_bounds.to_file(country_bounds_path+\"_simple\",driver=\"GeoJSON\")\n",
    "    # draw transects at country polygon \n",
    "    transects = postprocess.draw_transects_polygon(\n",
    "        country_bounds,\n",
    "        transect_len,\n",
    "        300,\n",
    "        transect_dist,\n",
    "        transect_min_line_length,\n",
    "        sigma=3,\n",
    "        out_path_poly=country_bounds_path+\"_smooth\"\n",
    "        )\n",
    "    # clip transects to buffer \n",
    "    transects = gpd.clip(transects,buffer)\n",
    "    transects = transects.dropna() # if transects have been created along a multipolygon\n",
    "    transects = transects.explode().reset_index(drop=True)\n",
    "    transects.to_file(transects_path,driver=\"GeoJSON\")\n",
    "    print(\"Transects have been created and saved.\")\n",
    "else:\n",
    "    # clip transects to min water extent raster\n",
    "    if not os.path.exists(transects_path+\"_clip\"):\n",
    "        print(\"Clip transects to min water extent...\")\n",
    "        min_water_buffer = min_water_poly.buffer(100)\n",
    "        transects_clip = gpd.clip(transects,min_water_buffer)\n",
    "        # convert all mutlilinestrings to single linestrings to treat transect pieces separately\n",
    "        transects_clip = transects_clip.explode().reset_index()\n",
    "        transects_clip.to_file(transects_path+\"_clip\",driver=\"GPKG\")\n",
    "        print(\"Transects haven been clipped to min water extent polygon and saved.\")\n",
    "    else:\n",
    "        print(\"Clipped transects already exist.\")"
   ]
  },
  {
   "source": [
    "### 7.| Calculate intersections between shorelines and transects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transects have been loaded.\n",
      "-------------------- Treating Sylt --------------------\n",
      "Calcualte intersections...\n",
      "0 intersected\n",
      "1 intersected\n",
      "2 intersected\n",
      "3 intersected\n",
      "4 intersected\n",
      "5 intersected\n",
      "6 intersected\n",
      "7 intersected\n",
      "8 intersected\n",
      "9 intersected\n",
      "10 intersected\n",
      "11 intersected\n",
      "12 intersected\n",
      "13 intersected\n",
      "14 intersected\n",
      "15 intersected\n",
      "16 intersected\n",
      "17 intersected\n",
      "18 intersected\n",
      "19 intersected\n",
      "20 intersected\n",
      "21 intersected\n",
      "22 intersected\n",
      "23 intersected\n",
      "24 intersected\n",
      "25 intersected\n",
      "26 intersected\n",
      "27 intersected\n",
      "28 intersected\n",
      "29 intersected\n",
      "30 intersected\n",
      "31 intersected\n",
      "32 intersected\n",
      "33 intersected\n",
      "34 intersected\n",
      "35 intersected\n",
      "36 intersected\n",
      "37 intersected\n",
      "38 intersected\n",
      "39 intersected\n",
      "40 intersected\n",
      "41 intersected\n",
      "42 intersected\n",
      "43 intersected\n",
      "44 intersected\n",
      "45 intersected\n",
      "46 intersected\n",
      "47 intersected\n",
      "48 intersected\n",
      "49 intersected\n",
      "50 intersected\n",
      "51 intersected\n",
      "52 intersected\n",
      "53 intersected\n",
      "54 intersected\n",
      "55 intersected\n",
      "56 intersected\n",
      "57 intersected\n",
      "58 intersected\n",
      "59 intersected\n",
      "60 intersected\n",
      "61 intersected\n",
      "62 intersected\n",
      "63 intersected\n",
      "64 intersected\n",
      "65 intersected\n",
      "66 intersected\n",
      "67 intersected\n",
      "68 intersected\n",
      "69 intersected\n",
      "70 intersected\n",
      "71 intersected\n",
      "72 intersected\n",
      "73 intersected\n",
      "74 intersected\n",
      "75 intersected\n",
      "76 intersected\n",
      "77 intersected\n",
      "78 intersected\n",
      "79 intersected\n",
      "80 intersected\n",
      "81 intersected\n",
      "82 intersected\n",
      "83 intersected\n",
      "84 intersected\n",
      "85 intersected\n",
      "86 intersected\n",
      "87 intersected\n",
      "88 intersected\n",
      "89 intersected\n",
      "90 intersected\n",
      "91 intersected\n",
      "92 intersected\n",
      "93 intersected\n",
      "94 intersected\n",
      "95 intersected\n",
      "96 intersected\n",
      "97 intersected\n",
      "98 intersected\n",
      "99 intersected\n",
      "100 intersected\n",
      "101 intersected\n",
      "102 intersected\n",
      "103 intersected\n",
      "104 intersected\n",
      "105 intersected\n",
      "106 intersected\n",
      "107 intersected\n",
      "108 intersected\n",
      "109 intersected\n",
      "110 intersected\n",
      "111 intersected\n",
      "112 intersected\n",
      "113 intersected\n",
      "114 intersected\n",
      "115 intersected\n",
      "116 intersected\n",
      "117 intersected\n",
      "118 intersected\n",
      "119 intersected\n",
      "120 intersected\n",
      "121 intersected\n",
      "122 intersected\n",
      "123 intersected\n",
      "124 intersected\n",
      "125 intersected\n",
      "126 intersected\n",
      "127 intersected\n",
      "128 intersected\n",
      "129 intersected\n",
      "130 intersected\n",
      "131 intersected\n",
      "132 intersected\n",
      "133 intersected\n",
      "134 intersected\n",
      "135 intersected\n",
      "136 intersected\n",
      "137 intersected\n",
      "138 intersected\n",
      "139 intersected\n",
      "140 intersected\n",
      "141 intersected\n",
      "142 intersected\n",
      "143 intersected\n",
      "144 intersected\n",
      "145 intersected\n",
      "146 intersected\n",
      "147 intersected\n",
      "148 intersected\n",
      "149 intersected\n",
      "150 intersected\n",
      "151 intersected\n",
      "152 intersected\n",
      "153 intersected\n",
      "154 intersected\n",
      "155 intersected\n",
      "156 intersected\n",
      "157 intersected\n",
      "158 intersected\n",
      "159 intersected\n",
      "160 intersected\n",
      "161 intersected\n",
      "162 intersected\n",
      "163 intersected\n",
      "164 intersected\n",
      "165 intersected\n",
      "166 intersected\n",
      "167 intersected\n",
      "168 intersected\n",
      "169 intersected\n",
      "170 intersected\n",
      "171 intersected\n",
      "172 intersected\n",
      "173 intersected\n",
      "174 intersected\n",
      "175 intersected\n",
      "176 intersected\n",
      "177 intersected\n",
      "178 intersected\n",
      "179 intersected\n",
      "180 intersected\n",
      "181 intersected\n",
      "182 intersected\n",
      "183 intersected\n",
      "184 intersected\n",
      "185 intersected\n",
      "186 intersected\n",
      "187 intersected\n",
      "188 intersected\n",
      "189 intersected\n",
      "190 intersected\n",
      "191 intersected\n",
      "192 intersected\n",
      "193 intersected\n",
      "194 intersected\n",
      "195 intersected\n",
      "196 intersected\n",
      "197 intersected\n",
      "198 intersected\n",
      "199 intersected\n",
      "200 intersected\n",
      "201 intersected\n",
      "202 intersected\n",
      "203 intersected\n",
      "204 intersected\n",
      "205 intersected\n",
      "206 intersected\n",
      "207 intersected\n",
      "208 intersected\n",
      "209 intersected\n",
      "210 intersected\n",
      "211 intersected\n",
      "212 intersected\n",
      "213 intersected\n",
      "214 intersected\n",
      "215 intersected\n",
      "216 intersected\n",
      "217 intersected\n",
      "218 intersected\n",
      "219 intersected\n",
      "220 intersected\n",
      "221 intersected\n",
      "222 intersected\n",
      "223 intersected\n",
      "224 intersected\n",
      "225 intersected\n",
      "226 intersected\n",
      "227 intersected\n",
      "228 intersected\n",
      "229 intersected\n",
      "230 intersected\n",
      "231 intersected\n",
      "232 intersected\n",
      "233 intersected\n",
      "234 intersected\n",
      "235 intersected\n",
      "236 intersected\n",
      "237 intersected\n",
      "238 intersected\n",
      "239 intersected\n",
      "240 intersected\n",
      "241 intersected\n",
      "242 intersected\n",
      "243 intersected\n",
      "244 intersected\n",
      "245 intersected\n",
      "246 intersected\n",
      "247 intersected\n",
      "248 intersected\n",
      "249 intersected\n",
      "250 intersected\n",
      "251 intersected\n",
      "252 intersected\n",
      "253 intersected\n",
      "254 intersected\n",
      "255 intersected\n",
      "256 intersected\n",
      "257 intersected\n",
      "258 intersected\n",
      "259 intersected\n",
      "260 intersected\n",
      "261 intersected\n",
      "262 intersected\n",
      "263 intersected\n",
      "264 intersected\n",
      "265 intersected\n",
      "266 intersected\n",
      "267 intersected\n",
      "268 intersected\n",
      "269 intersected\n",
      "270 intersected\n",
      "271 intersected\n",
      "272 intersected\n",
      "273 intersected\n",
      "274 intersected\n",
      "275 intersected\n",
      "276 intersected\n",
      "277 intersected\n",
      "278 intersected\n",
      "279 intersected\n",
      "280 intersected\n",
      "281 intersected\n",
      "282 intersected\n",
      "283 intersected\n",
      "284 intersected\n",
      "285 intersected\n",
      "286 intersected\n",
      "287 intersected\n",
      "288 intersected\n",
      "289 intersected\n",
      "290 intersected\n",
      "291 intersected\n",
      "292 intersected\n",
      "293 intersected\n",
      "294 intersected\n",
      "295 intersected\n",
      "296 intersected\n",
      "297 intersected\n",
      "298 intersected\n",
      "299 intersected\n",
      "300 intersected\n",
      "301 intersected\n",
      "302 intersected\n",
      "303 intersected\n",
      "304 intersected\n",
      "305 intersected\n",
      "306 intersected\n",
      "307 intersected\n",
      "308 intersected\n",
      "309 intersected\n",
      "310 intersected\n",
      "311 intersected\n",
      "312 intersected\n",
      "313 intersected\n",
      "314 intersected\n",
      "315 intersected\n",
      "316 intersected\n",
      "317 intersected\n",
      "318 intersected\n",
      "319 intersected\n",
      "320 intersected\n",
      "321 intersected\n",
      "322 intersected\n",
      "323 intersected\n",
      "324 intersected\n",
      "325 intersected\n",
      "326 intersected\n",
      "327 intersected\n",
      "328 intersected\n",
      "329 intersected\n",
      "330 intersected\n",
      "331 intersected\n",
      "332 intersected\n",
      "333 intersected\n",
      "334 intersected\n",
      "335 intersected\n",
      "336 intersected\n",
      "337 intersected\n",
      "338 intersected\n",
      "339 intersected\n",
      "340 intersected\n",
      "341 intersected\n",
      "342 intersected\n",
      "343 intersected\n",
      "344 intersected\n",
      "345 intersected\n",
      "346 intersected\n",
      "347 intersected\n",
      "348 intersected\n",
      "349 intersected\n",
      "350 intersected\n",
      "351 intersected\n",
      "352 intersected\n",
      "353 intersected\n",
      "354 intersected\n",
      "355 intersected\n",
      "356 intersected\n",
      "357 intersected\n",
      "358 intersected\n",
      "359 intersected\n",
      "360 intersected\n",
      "361 intersected\n",
      "362 intersected\n",
      "363 intersected\n",
      "364 intersected\n",
      "365 intersected\n",
      "366 intersected\n",
      "367 intersected\n",
      "368 intersected\n",
      "369 intersected\n",
      "370 intersected\n",
      "371 intersected\n",
      "372 intersected\n",
      "373 intersected\n",
      "374 intersected\n",
      "375 intersected\n",
      "376 intersected\n",
      "377 intersected\n",
      "378 intersected\n",
      "379 intersected\n",
      "380 intersected\n",
      "381 intersected\n",
      "382 intersected\n",
      "383 intersected\n",
      "384 intersected\n",
      "385 intersected\n",
      "386 intersected\n",
      "387 intersected\n",
      "388 intersected\n",
      "389 intersected\n",
      "390 intersected\n",
      "391 intersected\n",
      "392 intersected\n",
      "393 intersected\n",
      "394 intersected\n",
      "395 intersected\n",
      "396 intersected\n",
      "397 intersected\n",
      "398 intersected\n",
      "399 intersected\n",
      "400 intersected\n",
      "401 intersected\n",
      "402 intersected\n",
      "403 intersected\n",
      "404 intersected\n",
      "405 intersected\n",
      "406 intersected\n",
      "407 intersected\n",
      "408 intersected\n",
      "409 intersected\n",
      "410 intersected\n",
      "411 intersected\n",
      "412 intersected\n",
      "413 intersected\n",
      "414 intersected\n",
      "415 intersected\n",
      "416 intersected\n",
      "417 intersected\n",
      "418 intersected\n",
      "419 intersected\n",
      "420 intersected\n",
      "421 intersected\n",
      "422 intersected\n",
      "423 intersected\n",
      "424 intersected\n",
      "425 intersected\n",
      "426 intersected\n",
      "427 intersected\n",
      "428 intersected\n",
      "429 intersected\n",
      "430 intersected\n",
      "431 intersected\n",
      "432 intersected\n",
      "433 intersected\n",
      "434 intersected\n",
      "435 intersected\n",
      "436 intersected\n",
      "437 intersected\n",
      "438 intersected\n",
      "439 intersected\n",
      "440 intersected\n",
      "441 intersected\n",
      "442 intersected\n",
      "443 intersected\n",
      "444 intersected\n",
      "445 intersected\n",
      "446 intersected\n",
      "447 intersected\n",
      "448 intersected\n",
      "449 intersected\n",
      "450 intersected\n",
      "451 intersected\n",
      "452 intersected\n",
      "453 intersected\n",
      "454 intersected\n",
      "455 intersected\n",
      "456 intersected\n",
      "457 intersected\n",
      "458 intersected\n",
      "459 intersected\n",
      "460 intersected\n",
      "461 intersected\n",
      "462 intersected\n",
      "463 intersected\n",
      "464 intersected\n",
      "465 intersected\n",
      "466 intersected\n",
      "467 intersected\n",
      "468 intersected\n",
      "469 intersected\n",
      "470 intersected\n",
      "471 intersected\n",
      "472 intersected\n",
      "473 intersected\n",
      "474 intersected\n",
      "475 intersected\n",
      "476 intersected\n",
      "477 intersected\n",
      "478 intersected\n",
      "479 intersected\n",
      "480 intersected\n",
      "481 intersected\n",
      "482 intersected\n",
      "483 intersected\n",
      "484 intersected\n",
      "485 intersected\n",
      "486 intersected\n",
      "487 intersected\n",
      "488 intersected\n",
      "489 intersected\n",
      "490 intersected\n",
      "491 intersected\n",
      "492 intersected\n",
      "493 intersected\n",
      "494 intersected\n",
      "495 intersected\n",
      "496 intersected\n",
      "497 intersected\n",
      "498 intersected\n",
      "499 intersected\n",
      "500 intersected\n",
      "501 intersected\n",
      "502 intersected\n",
      "503 intersected\n",
      "504 intersected\n",
      "505 intersected\n",
      "506 intersected\n",
      "507 intersected\n",
      "508 intersected\n",
      "509 intersected\n",
      "510 intersected\n",
      "511 intersected\n",
      "512 intersected\n",
      "513 intersected\n",
      "514 intersected\n",
      "515 intersected\n",
      "516 intersected\n",
      "517 intersected\n",
      "518 intersected\n",
      "519 intersected\n",
      "520 intersected\n",
      "521 intersected\n",
      "522 intersected\n",
      "523 intersected\n",
      "524 intersected\n",
      "525 intersected\n",
      "526 intersected\n",
      "527 intersected\n",
      "528 intersected\n",
      "529 intersected\n",
      "530 intersected\n",
      "531 intersected\n",
      "532 intersected\n",
      "533 intersected\n",
      "534 intersected\n",
      "535 intersected\n",
      "536 intersected\n",
      "537 intersected\n",
      "538 intersected\n",
      "539 intersected\n",
      "540 intersected\n",
      "541 intersected\n",
      "542 intersected\n",
      "543 intersected\n",
      "544 intersected\n",
      "545 intersected\n",
      "546 intersected\n",
      "547 intersected\n",
      "548 intersected\n",
      "549 intersected\n",
      "550 intersected\n",
      "551 intersected\n",
      "552 intersected\n",
      "553 intersected\n",
      "554 intersected\n",
      "555 intersected\n",
      "556 intersected\n",
      "557 intersected\n",
      "558 intersected\n",
      "559 intersected\n",
      "560 intersected\n",
      "561 intersected\n",
      "562 intersected\n",
      "563 intersected\n",
      "564 intersected\n",
      "565 intersected\n",
      "566 intersected\n",
      "567 intersected\n",
      "568 intersected\n",
      "569 intersected\n",
      "570 intersected\n",
      "571 intersected\n",
      "572 intersected\n",
      "573 intersected\n",
      "574 intersected\n",
      "575 intersected\n",
      "576 intersected\n",
      "577 intersected\n",
      "578 intersected\n",
      "579 intersected\n",
      "580 intersected\n",
      "581 intersected\n",
      "582 intersected\n",
      "583 intersected\n",
      "584 intersected\n",
      "585 intersected\n",
      "586 intersected\n",
      "587 intersected\n",
      "588 intersected\n",
      "589 intersected\n",
      "590 intersected\n",
      "591 intersected\n",
      "592 intersected\n",
      "593 intersected\n",
      "594 intersected\n",
      "595 intersected\n",
      "596 intersected\n",
      "597 intersected\n",
      "598 intersected\n",
      "599 intersected\n",
      "600 intersected\n",
      "601 intersected\n",
      "602 intersected\n",
      "603 intersected\n",
      "604 intersected\n",
      "605 intersected\n",
      "606 intersected\n",
      "607 intersected\n",
      "608 intersected\n",
      "609 intersected\n",
      "610 intersected\n",
      "611 intersected\n",
      "612 intersected\n",
      "613 intersected\n",
      "614 intersected\n",
      "615 intersected\n",
      "616 intersected\n",
      "617 intersected\n",
      "618 intersected\n",
      "619 intersected\n",
      "620 intersected\n",
      "621 intersected\n",
      "622 intersected\n",
      "623 intersected\n",
      "624 intersected\n",
      "625 intersected\n",
      "626 intersected\n",
      "627 intersected\n",
      "628 intersected\n",
      "629 intersected\n",
      "630 intersected\n",
      "631 intersected\n",
      "632 intersected\n",
      "633 intersected\n",
      "634 intersected\n",
      "635 intersected\n",
      "636 intersected\n",
      "637 intersected\n",
      "638 intersected\n",
      "639 intersected\n",
      "640 intersected\n",
      "641 intersected\n",
      "642 intersected\n",
      "643 intersected\n",
      "644 intersected\n",
      "645 intersected\n",
      "646 intersected\n",
      "647 intersected\n",
      "648 intersected\n",
      "649 intersected\n",
      "650 intersected\n",
      "651 intersected\n",
      "652 intersected\n",
      "653 intersected\n",
      "654 intersected\n",
      "655 intersected\n",
      "656 intersected\n",
      "657 intersected\n",
      "658 intersected\n",
      "659 intersected\n",
      "660 intersected\n",
      "661 intersected\n",
      "662 intersected\n",
      "663 intersected\n",
      "664 intersected\n",
      "665 intersected\n",
      "666 intersected\n",
      "667 intersected\n",
      "668 intersected\n",
      "669 intersected\n",
      "670 intersected\n",
      "671 intersected\n",
      "672 intersected\n",
      "673 intersected\n",
      "674 intersected\n",
      "675 intersected\n",
      "676 intersected\n",
      "677 intersected\n",
      "678 intersected\n",
      "679 intersected\n",
      "680 intersected\n",
      "681 intersected\n",
      "682 intersected\n",
      "683 intersected\n",
      "684 intersected\n",
      "685 intersected\n",
      "686 intersected\n",
      "687 intersected\n",
      "688 intersected\n",
      "689 intersected\n",
      "690 intersected\n",
      "691 intersected\n",
      "692 intersected\n",
      "693 intersected\n",
      "694 intersected\n",
      "695 intersected\n",
      "696 intersected\n",
      "697 intersected\n",
      "698 intersected\n",
      "699 intersected\n",
      "700 intersected\n",
      "701 intersected\n",
      "702 intersected\n",
      "703 intersected\n",
      "704 intersected\n",
      "705 intersected\n",
      "706 intersected\n",
      "707 intersected\n",
      "708 intersected\n",
      "709 intersected\n",
      "710 intersected\n",
      "711 intersected\n",
      "712 intersected\n",
      "713 intersected\n",
      "714 intersected\n",
      "715 intersected\n",
      "716 intersected\n",
      "717 intersected\n",
      "718 intersected\n",
      "719 intersected\n",
      "720 intersected\n",
      "721 intersected\n",
      "722 intersected\n",
      "723 intersected\n",
      "724 intersected\n",
      "725 intersected\n",
      "726 intersected\n",
      "727 intersected\n",
      "728 intersected\n",
      "729 intersected\n",
      "730 intersected\n",
      "731 intersected\n",
      "732 intersected\n",
      "733 intersected\n",
      "734 intersected\n",
      "735 intersected\n",
      "736 intersected\n",
      "737 intersected\n",
      "738 intersected\n",
      "739 intersected\n",
      "740 intersected\n",
      "741 intersected\n",
      "742 intersected\n",
      "743 intersected\n",
      "744 intersected\n",
      "745 intersected\n",
      "746 intersected\n",
      "747 intersected\n",
      "748 intersected\n",
      "749 intersected\n",
      "750 intersected\n",
      "751 intersected\n",
      "752 intersected\n",
      "753 intersected\n",
      "754 intersected\n",
      "755 intersected\n",
      "756 intersected\n",
      "757 intersected\n",
      "758 intersected\n",
      "759 intersected\n",
      "760 intersected\n",
      "761 intersected\n",
      "762 intersected\n",
      "763 intersected\n",
      "764 intersected\n",
      "765 intersected\n",
      "766 intersected\n",
      "767 intersected\n",
      "768 intersected\n",
      "769 intersected\n",
      "770 intersected\n",
      "771 intersected\n",
      "772 intersected\n",
      "773 intersected\n",
      "Intersections have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Calculate intersections\n",
    "# Load transects \n",
    "try:\n",
    "    transects = gpd.read_file(transects_path)\n",
    "    print(\"Transects have been loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Transects file does not exist.\")\n",
    "\n",
    "# Intersections\n",
    "print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "folder_path = os.path.join(data_dir,tile_name)\n",
    "intersections_file = os.path.join(data_dir,tile_name,tile_name+\"_intersections\")\n",
    "if not os.path.exists(intersections_file):\n",
    "    try:\n",
    "        shorelines = gpd.read_file(os.path.join(data_dir,tile_name,tile_name+\"_shorelines\"))\n",
    "    except FileNotFoundError:\n",
    "        print('Shorelines do not exist.')\n",
    "    else:        \n",
    "        print(\"Calcualte intersections...\")\n",
    "        #tile_poly = tile.geometry\n",
    "        #transects = gpd.clip(transects,tile_poly)\n",
    "        intersections = postprocess.compute_intersections(transects,shorelines,remove_outliers=False)\n",
    "        if not intersections.empty:\n",
    "            intersections.to_file(intersections_file,driver=\"GPKG\")\n",
    "            print(\"Intersections have been created and saved.\")\n",
    "        else:\n",
    "            print(\"No intersections available for\",tile_name)\n",
    "else:\n",
    "    print(\"Intersections already exist.\")"
   ]
  },
  {
   "source": [
    "### 8.| Coastline Change Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Hotspot analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifications saved.\n"
     ]
    }
   ],
   "source": [
    "# Create classification transects (Erosion, Accretion, Stable, etc.)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "classifications_path = os.path.join(data_dir,\"Sylt_all_classifications1\")\n",
    "if not os.path.exists(classifications_path):\n",
    "    intersections = gpd.read_file(os.path.join(data_dir,tile_name,tile_name+\"_intersections\"))\n",
    "    classifications = postprocess.calc_change_metrics(intersections,2,crs)\n",
    "    classifications.to_file(classifications_path,driver=\"GPKG\")\n",
    "    print(\"Classifications saved.\")\n",
    "else:\n",
    "    print(\"Classifications exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All hotspots identified.\n",
      "Hotspot files saved.\n"
     ]
    }
   ],
   "source": [
    "### THIS HAS NOT BEEN DONE FOR ALASKA YET. \n",
    "\n",
    "# Calculate accretion and erosion hotspots \n",
    "erosion_hotspots_path = os.path.join(data_dir,\"VN_erosion_hotspots\")\n",
    "accretion_hotspots_path = os.path.join(data_dir,\"VN_accretion_hotspots\")\n",
    "\n",
    "if not os.path.exists(erosion_hotspots_path+\"a\"):\n",
    "    # load an prepare classification file \n",
    "    classification = gpd.read_file(os.path.join(data_dir,\"VN_all_classifications\"))\n",
    "    classification = classification.sort_values(by=\"Transect_id\").reset_index(drop=True)\n",
    "    #classification = classification.replace(\"nan\",np.NaN)\n",
    "    classification = classification[classification[\"class_L1\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # set up while loop to find clustered erosion and accretion classification transects\n",
    "    yet_seen = []\n",
    "    erosion_hotspots = []\n",
    "    accretion_hotspots = []\n",
    "    # iterate through all classification transects\n",
    "    for t, transect in classification.iterrows():\n",
    "            # check if transect has already been evaluated \n",
    "            if not t in yet_seen:\n",
    "                cluster = []\n",
    "                # create cluster if the next transect or the transect after has the same class \n",
    "                while (transect.class_L1 == classification.iloc[t].class_L1) or (transect.class_L1 == classification.iloc[t+1].class_L1) :\n",
    "                    yet_seen.append(t)\n",
    "                    cluster.append(classification.iloc[t])\n",
    "                    t +=1\n",
    "                    # exist while loop if last transect id +2 has been reached\n",
    "                    if t+2 > len(classification):\n",
    "                        break\n",
    "                # if less than 20 transect share the same class, omit from hotspot analysis\n",
    "                if len(cluster)>20:\n",
    "                    # save only erosion and accretion clusters and convert to GeoDataFrame\n",
    "                    if cluster[0].class_L1 == \"Accretion\":\n",
    "                        gdf = gpd.GeoDataFrame(cluster)\n",
    "                        accretion_hotspots.append(gdf)\n",
    "                    elif cluster[0].class_L1 == \"Erosion\":\n",
    "                        gdf = gpd.GeoDataFrame(cluster)\n",
    "                        erosion_hotspots.append(gdf)\n",
    "    print(\"All hotspots identified.\")\n",
    "    # add cluster number to hotspot transects\n",
    "    for i, transect in enumerate(erosion_hotspots):\n",
    "        transect['cluster_no'] = i\n",
    "    for i, transect in enumerate(accretion_hotspots):\n",
    "        transect['cluster_no'] = i \n",
    "    # merge all hotspot dataframes\n",
    "    erosion_hotspots_gdf = pd.concat(erosion_hotspots)\n",
    "    accretion_hotspots_gdf = pd.concat(accretion_hotspots)\n",
    "    # save dataframes\n",
    "    erosion_hotspots_gdf.to_file(erosion_hotspots_path,driver=\"GPKG\")\n",
    "    accretion_hotspots_gdf.to_file(accretion_hotspots_path,driver=\"GPKG\")\n",
    "    print(\"Hotspot files saved.\")\n",
    "else:\n",
    "    print(\"Hotspot files exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extreme hotspot files exist.\n"
     ]
    }
   ],
   "source": [
    "# Filter extreme erosion and accretion hotspots\n",
    "extreme_erosion_hotspots_path = os.path.join(data_dir,\"VN_extreme_erosion_hotspots\")\n",
    "extreme_accretion_hotspots_path = os.path.join(data_dir,\"VN_extreme_accretion_hotspots\")\n",
    "if not os.path.exists(extreme_erosion_hotspots_path):\n",
    "    erosion_hotspots = gpd.read_file(os.path.join(data_dir,\"VN_erosion_hotspots\"))\n",
    "    accretion_hotspots = gpd.read_file(os.path.join(data_dir,\"VN_accretion_hotspots\"))\n",
    "    print(\"Mean erosion of extreme hotspots:\")\n",
    "    extreme_erosion_hotspots = postprocess.define_severe_hotspots(erosion_hotspots,-5,\"smaller\")\n",
    "    print(\"Mean accretion of extreme hotspots:\")\n",
    "    extreme_accretion_hotspots = postprocess.define_severe_hotspots(accretion_hotspots,5,\"bigger\")\n",
    "    # save files\n",
    "    extreme_erosion_hotspots.to_file(extreme_erosion_hotspots_path,driver=\"GPKG\")\n",
    "    extreme_accretion_hotspots.to_file(extreme_accretion_hotspots_path,driver=\"GPKG\")\n",
    "else:\n",
    "    print(\"Extreme hotspot files exist.\")"
   ]
  },
  {
   "source": [
    "#### Land area change"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coastal provinces are already clipped to buffer and read.\nLand area change in province:  VN717\nLand area file already exists.\nLand area change in province:  VN821\nLand area file already exists.\nLand area change in province:  VN811\nLand area file already exists.\nLand area change in province:  VN507\nLand area file already exists.\nLand area change in province:  VN715\nLand area file already exists.\nLand area change in province:  VN823\nLand area file already exists.\nLand area change in province:  VN501\nLand area file already exists.\nLand area change in province:  VN713\nLand area file already exists.\nLand area change in province:  VN405\nLand area file already exists.\nLand area change in province:  VN103\nLand area file already exists.\nLand area change in province:  VN701\nLand area file already exists.\nLand area change in province:  VN511\nLand area file already exists.\nLand area change in province:  VN813\nLand area file already exists.\nLand area change in province:  VN113\nLand area file already exists.\nLand area change in province:  VN403\nLand area file already exists.\nLand area change in province:  VN117\nLand area file already exists.\nLand area change in province:  VN705\nLand area file already exists.\nLand area change in province:  VN509\nLand area file already exists.\nLand area change in province:  VN407\nLand area file already exists.\nLand area change in province:  VN503\nLand area file already exists.\nLand area change in province:  VN505\nLand area file already exists.\nLand area change in province:  VN225\nLand area file already exists.\nLand area change in province:  VN409\nLand area file already exists.\nLand area change in province:  VN819\nLand area file already exists.\nLand area change in province:  VN115\nLand area file already exists.\nLand area change in province:  VN401\nLand area file already exists.\nLand area change in province:  VN411\nLand area file already exists.\nLand area change in province:  VN807\nLand area file already exists.\nLand area change in province:  VN817\nLand area file already exists.\n"
     ]
    }
   ],
   "source": [
    "# calcualte land area change in the coastal zone (5km buffer) as proportional change \n",
    "provinces_clip_path = os.path.join(data_dir,\"VN_coastal_provinces_clip\")\n",
    "provinces_path = os.path.join(data_dir,\"VN_coastal_provinces\")\n",
    "buffer_path = os.path.join(data_dir,\"VN_buffer_5km\")\n",
    "\n",
    "# buffer coastal provinces by 5km and clip to buffer \n",
    "if not os.path.exists(provinces_clip_path):\n",
    "    buffer = gpd.read_file(buffer_path)\n",
    "    provinces = gpd.read_file(provinces_path)\n",
    "    provinces = provinces.to_crs(crs)\n",
    "    provinces['geometry'] = provinces.geometry.buffer(5000)\n",
    "    provinces = gpd.clip(provinces,buffer)\n",
    "    provinces.to_file(provinces_clip_path,driver=\"GeoJSON\")\n",
    "else:\n",
    "    print(\"Coastal provinces are already clipped to buffer and read.\")\n",
    "    provinces = gpd.read_file(provinces_clip_path)\n",
    "\n",
    "# calcualte proportional land area change for each province \n",
    "for i in provinces.index:\n",
    "    name = provinces.ADM1_PCODE.iloc[i]\n",
    "    print(\"Land area change in province: \",name)\n",
    "    land_area_path = os.path.join(plot_dir,\"land_area_change_\"+name)\n",
    "    if not os.path.exists(land_area_path):\n",
    "        land_area = pd.DataFrame(columns=['year','land_area_percentage'])\n",
    "        land_area['year'] = range(1987,2022)\n",
    "        for year in range(1987,2022):\n",
    "            print(year)\n",
    "            binary_files = glob.glob(os.path.join(\"{path}/**/*{year}*bin.tif\".format(path=data_dir,year=year)))\n",
    "            binary_files.sort()\n",
    "            land_pixels, valid_pixels = [],[]\n",
    "            for file in binary_files:\n",
    "                # crop raster to area\n",
    "                area = provinces[provinces.index==i]\n",
    "                with rio.open(file) as src:\n",
    "                    try:\n",
    "                        out_image, out_transform = rio.mask.mask(src,area.geometry,crop=True,nodata=np.nan)\n",
    "                        out_meta = src.meta\n",
    "                        out_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": out_image.shape[1],\n",
    "                            \"width\": out_image.shape[2],\n",
    "                            \"transform\": out_transform,\n",
    "                            \"compress\":\"LZW\"})\n",
    "                        out_path = os.path.join(os.getcwd(),\"test_data\",str(year)+\"_\"+name+\"_\"+os.path.basename(file)[:3])\n",
    "                        with rio.open(out_path,\"w\",**out_meta) as dst:\n",
    "                            dst.write(out_image)\n",
    "                        show(out_image)\n",
    "                        im_rev = out_image.copy()\n",
    "                        im_rev[im_rev==0]=2\n",
    "                        im_rev[im_rev==1]=0\n",
    "                        im_rev[im_rev==2]=1\n",
    "                        valid_pixel = np.count_nonzero(~np.isnan(out_image))\n",
    "                        land_pixel = np.nansum(im_rev)#*30*30\n",
    "                        land_pixels.append(land_pixel)\n",
    "                        valid_pixels.append(valid_pixel)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            land_pixel_percentage = np.sum(land_pixels)/np.sum(valid_pixels)*100\n",
    "            print(\"\\nLand pixel percentage in\",str(year)+\":\", land_pixel_percentage,\"\\n\")\n",
    "            land_area.loc[land_area.year == year, \"land_area_percentage\"] = land_pixel_percentage\n",
    "            land_area.to_csv(land_area_name)\n",
    "    else:\n",
    "        print(\"Land area file already exists.\")"
   ]
  }
 ]
}