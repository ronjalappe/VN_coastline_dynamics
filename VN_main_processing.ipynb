{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('geo_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ba7df6755b32bab95c00c96c0cebd9d890b50759e401e4d74df893f436e34bc5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Shoreline extraction from binary Landsat images for Vietnam (1980-2021)\n",
    "\n",
    "This notebook extracts subpixel contours from binary land/ water images derived from Landsat imagery, which have been processing on the Google Earth Engine (*reference to Javascript-file*). \n",
    "\n",
    "Content of the notebook:\n",
    "+ Setup: Python libaries, Directories, etc.\n",
    "+ Contour extraction \n",
    "+ Removal of short LineStrings\n",
    "+ ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.| Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "import rasterio as rio \n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show \n",
    "from coasty import postprocess"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "data_dir = os.path.join(os.getcwd(),\"data\") # path to data-folder with aux data\n",
    "proc_tiles_path = os.path.join(os.path.join(data_dir,\"VN_processing_polygons\")) # path to processing tiles\n",
    "country_bounds_path = os.path.join(data_dir,\"VN_country_bounds\") # path to country bounds\n",
    "osm_sl_path = os.path.join(os.path.join(data_dir,\"VN_osm_coastline\")) # path to reference shoreline\n",
    "buffer_path = os.path.join(os.path.join(data_dir,\"VN_buffer\"))\n",
    "\n",
    "# Params\n",
    "export_folder = \"GEE\"               # folder on Google Drive with GEE images to download\n",
    "crs = \"EPSG:3857\"                   # coordinate system code of a projected crs \n",
    "min_length = 3000                   # min length of shoreline to keep [m]\n",
    "buffer_dist = 2500                  # buffer around reference shorelines to clip detected shorelines [m]\n",
    "transect_len = 5000                 # length of transects [m]\n",
    "transect_dist = 200                 # distance between transects [m]\n",
    "transect_min_line_length = 10000    # min legnth of polygon outline at which to draw transects [m]\n",
    "                                    # (for removing small islands) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read/ create aux data \n",
    "proc_tiles = gpd.read_file(proc_tiles_path).to_crs(crs)\n",
    "country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "osm_sl = gpd.read_file(osm_sl_path).to_crs(crs)\n",
    "\n",
    "if os.path.exists(buffer_path):\n",
    "    buffer = gpd.read_file(buffer_path)\n",
    "else:\n",
    "    print(\"Create osm shoreline buffer:\")\n",
    "    buffer = osm_sl.buffer(buffer_dist)\n",
    "    buffer.to_file(buffer_path,driver=\"GeoJSON\")\n",
    "\n",
    "print(\"Everything successfully read.\")"
   ]
  },
  {
   "source": [
    "### 2.| Download binary rasters from Google Drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "P0\nP1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nP10\n"
     ]
    }
   ],
   "source": [
    "# Loop through processing tiles and save in separate folers \n",
    "for i in proc_tiles.id:\n",
    "    tile_name = \"P\"+str(i) # name of processing tile\n",
    "    folder_path = os.path.join(data_dir,tile_name) # path to save the rasters\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    # download images of current tile form Drive \n",
    "    postprocess.download_from_drive(export_folder,folder_path,tile_name) \n",
    "    # move downloaded images to out_path folder \n",
    "    for raster in os.listdir(os.getcwd()):\n",
    "        if raster.endswith(tile_name+\".tif\"):\n",
    "            os.replace(os.path.join(os.getcwd(),raster),os.path.join(folder_path,raster))\n",
    "    print('Files moved to: data/',tile_name)"
   ]
  },
  {
   "source": [
    "### 3.| Reproject and quality check binary rasters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating P0 --------------------\n",
      "1_2000_L5_P0.tif already projected to given CRS.\n",
      "1_2000_L5_P0.tif masked.\n",
      "1_2000_L5_P0_03avg_aq.tif saved.\n"
     ]
    }
   ],
   "source": [
    "for i in proc_tiles.id:\n",
    "    tile_name = \"P\"+str(i)\n",
    "    print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "    folder_path = os.path.join(data_dir,tile_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        raster_paths = glob.glob(os.path.join(data_dir,tile_name,\"*\"+tile_name+\".tif\"))\n",
    "        raster_paths.sort()\n",
    "        for r in raster_paths:\n",
    "            raster_path = os.path.join(data_dir,folder_path,raster)\n",
    "            postprocess.reproject_raster(raster_path,raster_path,crs)\n",
    "            postprocess.mask_single_observation_pixel(raster_path)\n",
    "    else:\n",
    "        print(folder_path,\"does not exist.\")    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                geometry\n",
       "0      LINESTRING (11663577.388 1029554.452, 11663588...\n",
       "1      LINESTRING (11664280.754 1029603.379, 11664295...\n",
       "2      LINESTRING (11664746.810 1029617.825, 11664761...\n",
       "3      LINESTRING (11665829.559 1029617.531, 11665844...\n",
       "4      LINESTRING (11666264.076 1029602.202, 11666251...\n",
       "...                                                  ...\n",
       "37463  LINESTRING (11670746.294 942766.199, 11670775....\n",
       "37464  LINESTRING (11670669.358 942631.481, 11670654....\n",
       "37465  LINESTRING (11670533.463 941474.445, 11670521....\n",
       "37466  LINESTRING (11671977.970 939873.160, 11671988....\n",
       "37467  LINESTRING (11673112.371 940742.641, 11673124....\n",
       "\n",
       "[37468 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LINESTRING (11663577.388 1029554.452, 11663588...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LINESTRING (11664280.754 1029603.379, 11664295...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LINESTRING (11664746.810 1029617.825, 11664761...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LINESTRING (11665829.559 1029617.531, 11665844...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LINESTRING (11666264.076 1029602.202, 11666251...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>37463</th>\n      <td>LINESTRING (11670746.294 942766.199, 11670775....</td>\n    </tr>\n    <tr>\n      <th>37464</th>\n      <td>LINESTRING (11670669.358 942631.481, 11670654....</td>\n    </tr>\n    <tr>\n      <th>37465</th>\n      <td>LINESTRING (11670533.463 941474.445, 11670521....</td>\n    </tr>\n    <tr>\n      <th>37466</th>\n      <td>LINESTRING (11671977.970 939873.160, 11671988....</td>\n    </tr>\n    <tr>\n      <th>37467</th>\n      <td>LINESTRING (11673112.371 940742.641, 11673124....</td>\n    </tr>\n  </tbody>\n</table>\n<p>37468 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "shoreline"
   ]
  },
  {
   "source": [
    "### 2.| Subpixel contours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------- Treating P0 --------------------\n",
      "Buffer exists and has been loaded.\n",
      "Process shorelines...\n",
      "2000: Shoreline processed.\n",
      "2000: Shoreline processed.\n",
      "Shorelines have been created and saved.\n",
      "-------------------- Treating P1 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P2 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P3 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P4 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P5 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P6 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P7 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P8 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P9 --------------------\n",
      "Folder does not exist.\n",
      "-------------------- Treating P10 --------------------\n",
      "Folder does not exist.\n"
     ]
    }
   ],
   "source": [
    "for i in proc_tiles.id:\n",
    "    tile_name = \"P\"+str(i)\n",
    "    print((\"--\")*10,\"Treating\",tile_name,(\"--\")*10)\n",
    "    folder_path = os.path.join(data_dir,tile_name)\n",
    "    if os.path.exists(folder_path):\n",
    "\n",
    "        #  Clip osm shoreline buffer to processing tile \n",
    "        buffer_clip_path = os.path.join(data_dir,tile_name,tile_name+\"_buffer\")\n",
    "        if not os.path.exists(buffer_clip_path):\n",
    "            buffer_clip = gpd.clip(buffer,proc_tiles[proc_tiles.index == i])\n",
    "            buffer_clip.to_file(buffer_clip_path,driver=\"GeoJSON\")\n",
    "            print(\"Buffer has been saved.\")\n",
    "        else: \n",
    "            print(\"Buffer exists and has been loaded.\")\n",
    "            buffer_clip = gpd.read_file(buffer_clip_path)\n",
    "        \n",
    "        # Create shorelines\n",
    "        shorelines = []\n",
    "        shorelines_path = os.path.join(folder_path,tile_name+\"_shorelines\") \n",
    "        if not os.path.exists(shorelines_path):\n",
    "            print(\"Process shorelines...\")    \n",
    "            raster_paths = glob.glob(os.path.join(data_dir,\"*aq.tif\"))\n",
    "            raster_paths.sort()\n",
    "            for r in raster_paths:    \n",
    "                shoreline = subpixel_contours(r,30)\n",
    "                # save single shoreline without modifications as backup\n",
    "                sl_path = os.path.join(folder_path,tile_name+\"_single_shorelines\")\n",
    "                if not os.path.exists(sl_path): os.mkdir(sl_path)\n",
    "                shoreline.to_file(os.path.join(sl_path,os.path.basename(r)+\"_shoreline\"))\n",
    "                # postprocess shoreline\n",
    "                shoreline = gpd.clip(shoreline,buffer_clip)\n",
    "                cleaned = postprocess.remove_small_lines(shoreline, min_size=min_length)\n",
    "                if not cleaned.empty:\n",
    "                    year = os.path.basename(r)[2:6]\n",
    "                    sat = os.path.basename(r)[7:9]\n",
    "                    avg_aq = os.path.basename(r)[13:15]\n",
    "                    cleaned['id']=year\n",
    "                    cleaned = cleaned.dissolve(by=cleaned.id,aggfunc=\"sum\")\n",
    "                    cleaned['year']=year\n",
    "                    cleaned['sat']=sat\n",
    "                    cleaned['avg_aq']=avg_aq\n",
    "                    cleaned['proc_tile']=tile_name                        \n",
    "                    shorelines.append(cleaned)\n",
    "                    print(year+\": shoreline processed.\")\n",
    "            shorelines_gdf = pd.concat(shorelines,ignore_index=True)\n",
    "            shorelines_gdf.to_file(os.path.join(shorelines_path),driver=\"GeoJSON\")\n",
    "            print(\"All shorelines have been created and saved.\")\n",
    "        else:\n",
    "            print(\"Shorelines already exist.\")\n",
    "    else:\n",
    "        print('Folder does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Images for P1 exist.\n",
      "Buffer exists and has been loaded.\n",
      "Process shorelines for P1...\n",
      "1_1995_L5_P1_24aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1992_L5_P1_8aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1996_L5_P1_27aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1989_L5_P1_20aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1994_L5_P1_18aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1988_L5_P1_13aq.tif\n",
      "   Raster clipped\n",
      "1_1993_L5_P1_15aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1990_L5_P1_14aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "1_1991_L5_P1_10aq.tif\n",
      "   Raster reprojected\n",
      "   Raster clipped\n",
      "Shorelines have been created and saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# access raster files by proccessing tiles\n",
    "proc_tiles = gpd.read_file(proc_tiles_path)\n",
    "proc_tiles = proc_tiles.to_crs(crs)\n",
    "\n",
    "# create buffer around osm reference shoreline\n",
    "osm_sl = gpd.read_file(osm_sl_path)\n",
    "osm_sl = osm_sl.to_crs(crs)\n",
    "\n",
    "for i in range(0,len(proc_tiles)):\n",
    "    raster_folder_path = os.path.join(data_dir,\"P\"+str(i))\n",
    "    if os.path.exists(raster_folder_path):\n",
    "        (print(\"Images for P\"+str(i)+\" exist.\"))\n",
    "        shorelines = []\n",
    "        shorelines_path = os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_shorelines\")\n",
    "        #  clip osm reference shoreline and to AOI \n",
    "        buffer_path = os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_buffer\")\n",
    "        if not os.path.exists(buffer_path):\n",
    "            osm_sl_clip = gpd.clip(osm_sl,proc_tiles[proc_tiles.index == i])\n",
    "            buffer = osm_sl_clip.buffer(buffer_dist)\n",
    "            buffer.to_file(buffer_path,driver=\"GeoJSON\")\n",
    "            print(\"Buffer has been saved.\")\n",
    "        else: \n",
    "            print(\"Buffer exists and has been loaded.\")\n",
    "            buffer = gpd.read_file(buffer_path) \n",
    "        if not os.path.exists(shorelines_path):\n",
    "            print(\"Process shorelines for P\"+str(i)+\"...\")    \n",
    "            for r in os.listdir(raster_folder_path):\n",
    "                if r.endswith(\"aq.tif\"):\n",
    "                    print(r)\n",
    "                    raster_path = os.path.join(raster_folder_path,r)\n",
    "                    raster_path_reproj = os.path.join(raster_folder_path,os.path.splitext(r)[0]+\"_reproj.tif\")\n",
    "                    raster_path_clip = os.path.join(raster_folder_path,os.path.splitext(r)[0]+\"_clip.tif\")\n",
    "                    if not os.path.exists(raster_path_reproj):\n",
    "                        postprocess.reproject_raster(raster_path,raster_path_reproj,crs)\n",
    "                        print(\"   Raster reprojected\")\n",
    "                    if not os.path.exists(raster_path_clip):\n",
    "                        postprocess.crop_raster(raster_path_reproj,test_box_path,raster_path_clip)\n",
    "                        print(\"   Raster clipped\")\n",
    "                    raster = rio.open(raster_path_reproj)\n",
    "                    shoreline = postprocess.subpixel_contours(raster,30)\n",
    "                    cleaned = postprocess.remove_small_lines(shoreline, min_size=min_length)\n",
    "                    if not cleaned.empty:\n",
    "                        year = r[2:6]\n",
    "                        sat = r[7:9]\n",
    "                        proc_tile = r[10:12]\n",
    "                        cleaned['id']=year\n",
    "                        cleaned = cleaned.dissolve(by=cleaned.id,aggfunc=\"sum\")\n",
    "                        cleaned['year']=year\n",
    "                        cleaned['sat']=sat\n",
    "                        cleaned['proc_tile']=proc_tile                        \n",
    "                        shorelines.append(cleaned)\n",
    "                        print(\"   Shoreline processed\")\n",
    "            shorelines_gdf = pd.concat(shorelines,ignore_index=True)\n",
    "            \n",
    "            shorelines_gdf.to_file(os.path.join(shorelines_path),driver=\"GeoJSON\")\n",
    "            print(\"Shorelines have been created and saved.\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"Shorelines for P\"+str(i)+\" exist.\\n\")"
   ]
  },
  {
   "source": [
    "### 3.| Transects between highest and lowest water extent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1995_L5_P1_24aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1993_L5_P1_15aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1996_L5_P1_27aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1991_L5_P1_10aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1992_L5_P1_8aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1988_L5_P1_13aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1990_L5_P1_14aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1994_L5_P1_18aq_reproj.tif\n",
      "Eating file: /Users/Ronjamac/Documents/02_Studium/Masterarbeit/Code/VN_coastline_dynamics/test_data/P1/1_1989_L5_P1_20aq_reproj.tif\n"
     ]
    }
   ],
   "source": [
    "# Calculate raster with min and max water extent \n",
    "\n",
    "def calc_water_extent(files_list,min_file,max_file):\n",
    "    all_masks = None\n",
    "    for idx, file in enumerate(files):#\n",
    "        print(\"Eating file: %s\" % file)\n",
    "        with rio.open(file, \"r\") as src:  # src has meta that can be accessed through \n",
    "                                            # src.meta or directly, e.g. src.height\n",
    "            if all_masks is None:  # we have not defined it yet but we only have do define ones\n",
    "                all_masks = np.zeros((len(files), src.height, src.width), dtype=np.float32)  # np.float32 may have nans\n",
    "                meta = src.meta\n",
    "            all_masks[idx] = src.read(1)                    \n",
    "    min_water_extent = np.nanmin(all_masks, 0)  # water = 1, min water extent\n",
    "    max_water_extent = np.nanmax(all_masks, 0)  # no water = 0, max water extent\n",
    "    # write the masks\n",
    "    for arr, out_file in zip([min_water_extent, max_water_extent], [min_water_file, max_water_file]):\n",
    "        with rio.open(out_file, \"w\", **meta) as tgt:\n",
    "            tgt.write(arr,1)\n",
    "\n",
    "for i in range(len(proc_tiles)):\n",
    "    raster_folder_path = os.path.join(data_dir,\"P\"+str(i))\n",
    "    if os.path.exists(raster_folder_path):\n",
    "        files = glob.glob(os.path.join(raster_folder_path,\"*reproj.tif\"))\n",
    "        min_water_file = os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_min_water_extent\")\n",
    "        max_water_file = os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_max_water_extent\")\n",
    "        if not os.path.exists(min_water_file):\n",
    "            calc_water_extent(files,min_water_file,max_water_file)\n",
    "        else:\n",
    "            print(\"Files exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte shoreline for min and max water extent rasters (might be useful at one point :))\n",
    "min_water_file = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_min_water_extent\")\n",
    "max_water_file = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_max_water_extent\")\n",
    "min_raster = rio.open(min_water_file)\n",
    "max_raster = rio.open(max_water_file)\n",
    "\n",
    "#test_transects = gpd.read_file(os.path.join(data_dir,\"transects_clip\"))\n",
    "\n",
    "#min_sl = postprocess.subpixel_contours(min_raster,30)\n",
    "#min_sl = postprocess.remove_small_lines(min_sl,min_length)\n",
    "#min_sl = gpd.clip(min_sl,buffer)\n",
    "#max_sl = postprocess.subpixel_contours(max_raster,30)\n",
    "#max_sl = postprocess.remove_small_lines(max_sl,min_length)\n",
    "#max_sl = gpd.clip(max_sl,buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove small pixel clusters\n",
    "import skimage\n",
    "\n",
    "def remove_pixel_cluster(raster_path,out_path,min_size1,min_size0,connectivity=0):\n",
    "    with rio.open(raster_path,'r') as src:\n",
    "        im = src.read(1)\n",
    "        meta = src.meta\n",
    "        im_rev = im.copy()\n",
    "        im_rev[im_rev==0]=2\n",
    "        im_rev[im_rev==1]=0\n",
    "        im_rev[im_rev==2]=1\n",
    "        processed_rev = skimage.morphology.remove_small_objects(im_rev.astype(bool),min_size=min_size1,connectivity=connectivity).astype('int16')\n",
    "        im[processed_rev==0]=1\n",
    "        processed = skimage.morphology.remove_small_objects(im.astype(bool), min_size=min_size0, connectivity=connectivity).astype('int16')\n",
    "        # black out pixels\n",
    "        #mask_x, mask_y = np.where(processed == 1)\n",
    "        #im[mask_x, mask_y] = 1\n",
    "        # plot the result\n",
    "        #plt.figure(figsize=(20,10))\n",
    "        #plt.imshow(processed)\n",
    "        meta.update({\n",
    "            \"compress\":\"LZW\",\n",
    "            \"dtype\":\"int16\"\n",
    "        })\n",
    "        with rio.open(out_path,'w',**meta) as dst:\n",
    "            dst.write(processed, 1)\n",
    "\n",
    "# polygonize min water raster \n",
    "def polygonize_raster(raster_path,raster_value,min_length):\n",
    "    # Read input band with Rasterio\n",
    "    with rio.open(raster_path) as src:\n",
    "        crs = src.crs\n",
    "        src_band = src.read(1)\n",
    "        #src_band[src_band==0]=2\n",
    "        #src_band[src_band==1]=0\n",
    "        #src_band[src_band==2]=1\n",
    "        # Polygonize with Rasterio. `shapes()` returns an iterable\n",
    "        # of (geom, value) as tuples\n",
    "        shapes = list(rio.features.shapes(src_band, transform=src.transform))\n",
    "    shp_schema = {\n",
    "        'geometry': 'MultiPolygon',\n",
    "        'properties': {'pixelvalue': 'int'}\n",
    "        }\n",
    "    # keep polygons with specified raster pixel value    \n",
    "    polygons = [shp.geometry.shape(geom) for geom, value in shapes\n",
    "                if value == raster_value]\n",
    "    # save polygons as geodataframe\n",
    "    polygons_gdf = gpd.GeoDataFrame(geometry=polygons,crs=crs)\n",
    "    # remove small polygons with given min length\n",
    "    #polygons_gdf = postprocess.remove_small_lines(polygons_gdf,min_length)\n",
    "    return polygons_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pixel cluster removed.\n",
      "Minimum water extent polygon created.\n",
      "Minimum water extent polygon buffered.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test polyonize min water raster with clip images:\n",
    "#min_water_file_clip = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_min_water_extent_clip\")\n",
    "#max_water_file_clip = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_max_water_extent_clip\")\n",
    "min_water_file = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_min_water_extent\")\n",
    "min_water_file_out = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_min_water_extent_simple\")\n",
    "\n",
    "#max_water_file = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_max_water_extent\")\n",
    "#max_water_file_out = os.path.join(data_dir,\"P\"+str(1),\"P\"+str(1)+\"_max_water_extent_simple\")\n",
    "\n",
    "remove_pixel_cluster(min_water_file,min_water_file_out,50000,100000,0)\n",
    "print(\"Pixel cluster removed.\")\n",
    "min_water_poly = polygonize_raster(min_water_file_out,0,10000) #current:10000\n",
    "min_water_poly.to_file(os.path.join(data_dir,\"P1\",\"P1_min_water_poly_simple\"),driver=\"GeoJSON\")\n",
    "print(\"Minimum water extent polygon created.\")\n",
    "\n",
    "# clip transects to min and max water extent polygons\n",
    "proc_tiles = gpd.read_file(proc_tiles_path)\n",
    "proc_tiles = proc_tiles.to_crs(crs)\n",
    "country_bounds = gpd.read_file(country_bounds_path).to_crs(crs)\n",
    "tile1 = proc_tiles.geometry.iloc[1]\n",
    "bounds = gpd.clip(country_bounds,tile1)#.buffer(buffer_dist)\n",
    "transects = gpd.read_file(country_transects_path)\n",
    "transects = gpd.clip(transects,tile1)\n",
    "\n",
    "min_water_buffer = min_water_poly.buffer(200)\n",
    "min_water_buffer.to_file(os.path.join(data_dir,\"P1\",\"P1_min_water_poly_simple_buffer\"),driver=\"GeoJSON\")\n",
    "print(\"Minimum water extent polygon buffered.\")\n",
    "\n",
    "transects_trim = gpd.clip(transects,min_water_buffer)\n",
    "\n",
    "transects_trim = transects_trim.explode().reset_index(drop=True)\n",
    "transects_trim = transects_trim.drop_duplicates(subset=\"id\",keep=\"last\")\n",
    "transects_trim.to_file(os.path.join(data_dir,\"P1\",\"P1_transects\"),driver=\"GeoJSON\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "source": [
    "### 3.| Time Series Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transects for Vietnam exist and have been loaded.\n",
      "Calcualte intersections for P1_shorelines...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Transects \n",
    "proc_tiles = gpd.read_file(proc_tiles_path)\n",
    "proc_tiles = proc_tiles.to_crs(crs)\n",
    "\n",
    "if not os.path.exists(country_transects_path):\n",
    "    country_bounds = gpd.read_file(country_bounds_path)\n",
    "    country_bounds = country_bounds.to_crs(crs)\n",
    "    country_transects = postprocess.draw_transects_polygon(country_bounds,transect_len,transect_dist,transect_min_line_length)\n",
    "    country_transects.to_file(country_transects_path,driver=\"GeoJSON\")\n",
    "    print(\"Transects for Vietnam have been created and saved.\")\n",
    "else:\n",
    "    country_transects = gpd.read_file(country_transects_path)\n",
    "    print(\"Transects for Vietnam exist and have been loaded.\")\n",
    "\n",
    "# Intersections\n",
    "for i, tile in proc_tiles.iterrows():\n",
    "    shorelines_path = os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_shorelines\")\n",
    "    \n",
    "    if os.path.exists(shorelines_path):\n",
    "        shorelines_gdf = gpd.read_file(shorelines_path)\n",
    "        print(\"Calcualte intersections for\", os.path.basename(shorelines_path)+\"...\")\n",
    "        tile_poly = tile.geometry\n",
    "        #test_box = gpd.read_file(test_box_path)\n",
    "        #test_box = test_box.to_crs(crs)\n",
    "        transects = gpd.clip(country_transects,tile_poly)\n",
    "        #osm_sl = gpd.clip(osm_sl,test_box)\n",
    "        intersections = compute_intersections(transects,shorelines_gdf,remove_outliers=True)\n",
    "        intersections.to_file(os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_intersections\"),driver=\"GeoJSON\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_intersections(transects, shorelines, remove_outliers=False,reference=None):\n",
    "    \"\"\"This functions calculates intersections between shore-perpendicular transects and a GeoDataFrame with shorelines. \n",
    "    It calculates the distance of the each intersection point to the origin of the transects and adds it as a property to \n",
    "    the output intersections GeoDataFrame. If the parameter \"reference\" is given, the distance of each intersection point \n",
    "    to a reference shoreline is calculated additionally in order to only keep the intersection point of each year which is \n",
    "    closest to the reference line.\n",
    "\n",
    "    Args:\n",
    "        transects (GeoDataFrame): with LineStrings [required column: \"transect_id\"]\n",
    "        shorelines (GeoDataFrame): with LineStrings and/ or MultiLineStrings [recommended column: \"year\"]\n",
    "        reference (GeoDataFrame, optional): with LineStrings and/ or MultiLineString. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: with Points and information on location, transect number, (year) and distance to\n",
    "    transect origin and the reference line, if given.\n",
    "    \"\"\"\n",
    "    # crs\n",
    "    crs = shorelines.crs\n",
    "    transects = transects.to_crs(crs)\n",
    "    if reference is not None:\n",
    "        reference = reference.to_crs(crs)\n",
    "        ref_inter = compute_intersections(transects, reference)\n",
    "    # empty list to store point dataframes for all transects\n",
    "    all_intersections = []\n",
    "    # loop through all transects and compute intersections\n",
    "    for t, transect in transects.iterrows():\n",
    "        transect = transect.geometry\n",
    "        transect_id = t\n",
    "        # empty list to store intersection points dataframe for each transect\n",
    "        intersections = []\n",
    "        # loop through all shorelines \n",
    "        for s, shoreline in shorelines.iterrows():\n",
    "            shoreline = shoreline.geometry\n",
    "            inter = []\n",
    "            # handle single linestrings\n",
    "            if type(shoreline)==shp.geometry.linestring.LineString:\n",
    "                inter.append(transect.intersection(shoreline))\n",
    "            # handle mutlilinestrings\n",
    "            elif type(shoreline)==shp.geometry.multilinestring.MultiLineString:\n",
    "                for sh in shoreline:\n",
    "                    inter.append(transect.intersection(sh))\n",
    "            # create geodataframe from list of intersection points \n",
    "            gdf = gpd.GeoDataFrame(geometry=inter, crs=crs)\n",
    "            # add transect id\n",
    "            gdf['transect_id'] = transect_id\n",
    "            # add year\n",
    "            if 'year' in shorelines:\n",
    "                gdf['year'] = shorelines.year.loc[s]\n",
    "            # add to list\n",
    "            intersections.append(gdf)\n",
    "        # merge dataframes of each transect to one \n",
    "        intersections_gdf =  pd.concat(intersections,ignore_index=True)\n",
    "        # drop empty geometries\n",
    "        intersections_gdf = intersections_gdf[~intersections_gdf.is_empty].reset_index(drop=True)\n",
    "        # seperate Multipoint geometries \n",
    "        intersections_gdf = intersections_gdf.explode()\n",
    "        # calculate the distance of intersections points to the (landwards) origin of the transect\n",
    "        dist = []\n",
    "        for i, inter in intersections_gdf.iterrows():\n",
    "            origin = shp.geometry.Point(transect.coords[1])\n",
    "            dist.append(origin.distance(inter.geometry))\n",
    "        # add distance information to dataframe\n",
    "        intersections_gdf['dist_to_transect_origin'] = dist\n",
    "        \n",
    "        ### 1. OPTION: CALCULATE DISTANCE TO REFERENCE SHORELINE AND SELECT POINT \n",
    "        # additionally calculate distance to reference shoreline\n",
    "        if reference is not None:\n",
    "            dist_to_osm_sl = []\n",
    "            for p, point in intersections_gdf.iterrows():\n",
    "                sl_point = point.geometry\n",
    "                osm_point = ref_inter[ref_inter.transect_id==point.transect_id].geometry.iloc[0]\n",
    "                dist = sl_point.distance(osm_point)\n",
    "                dist_to_osm_sl.append(dist)\n",
    "            intersections_gdf[\"dist_to_osm_sl\"] = dist_to_osm_sl\n",
    "            intersections_gdf = intersections_gdf.sort_values(by=\"dist_to_osm_sl\")\n",
    "            intersections_gdf = intersections_gdf.drop_duplicates(subset=\"year\",keep=\"first\")\n",
    "\n",
    "        ### 2. OPTION: CALCULATE DISTANCE TO MEDIAN INTERSECTION POINT AND SELECT UPON    \n",
    "        # calculate the median distance to the origin of the reference shoreline\n",
    "        #median_dist = np.median(intersections_gdf.dist_to_transect_origin)\n",
    "        #intersections_gdf['change'] = intersections_gdf.dist_to_transect_origin - median_dist\n",
    "        # create new column with absolute change to identify outliers\n",
    "        #intersections_gdf['abs_change'] = abs(intersections_gdf.change)\n",
    "        # drop duplicates. keep only one point per year which is closest to the median \n",
    "        #intersections_gdf = intersections_gdf.sort_values(by=\"abs_change\")\n",
    "        #intersections_gdf = intersections_gdf.drop_duplicates(subset=\"year\",keep=\"first\")\n",
    "\n",
    "\n",
    "        ### 3. OPTION: CHOOSE THE OUTERMOST POINT IN SEAWARDS DIRECTION \n",
    "        intersections_gdf = intersections_gdf.sort_values(by=\"dist_to_transect_origin\")\n",
    "        intersections_gdf = intersections_gdf.drop_duplicates(subset=\"year\",keep=\"last\")\n",
    "        \n",
    "        # remove outliers \n",
    "        if remove_outliers == True:\n",
    "            inter_median = np.median(intersections_gdf.dist_to_transect_origin)\n",
    "            inter_std =  np.std(intersections_gdf.dist_to_transect_origin)\n",
    "            intersections_gdf = intersections_gdf[intersections_gdf.dist_to_transect_origin.map(\n",
    "                    lambda x: abs(x-inter_median))<abs(3*inter_std)]\n",
    "        \n",
    "        # sort dataframe by date\n",
    "        if 'year' in intersections_gdf:\n",
    "            intersections_gdf = intersections_gdf.sort_values(by=\"year\")\n",
    "        # add dataframe to list\n",
    "        all_intersections.append(intersections_gdf)\n",
    "    # merge all dataframes\n",
    "    new_gdf = pd.concat(all_intersections,ignore_index=True)\n",
    "    new_gdf = new_gdf.to_crs(crs)\n",
    "    return new_gdf\n",
    "    \n",
    "#transects = transects.reset_index(drop=True)\n",
    "#i = 1\n",
    "#transects_trim = gpd.read_file(os.path.join(data_dir,\"P1\",\"P1_transects\"))\n",
    "#shorelines_gdf = gpd.read_file(os.path.join(data_dir,\"P1\",\"P1_shorelines\"))\n",
    "\n",
    "#transects_trim = transects_trim.explode().reset_index(drop=True)\n",
    "#test = compute_intersections(transects_trim,shorelines_gdf)\n",
    "#test.to_file(os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_intersections\"),driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "intersections = compute_intersections(transects_trim,shorelines_gdf)\n",
    "intersections.to_file(os.path.join(data_dir,\"P\"+str(i),\"P\"+str(i)+\"_intersections\"),driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Test intersections function\n",
    "transects_clip_file = os.path.join(data_dir,\"P1\",\"P1_transects_clip\")\n",
    "intersections_clip_file = os.path.join(data_dir,\"P1\",\"P1_clip_intersections\")\n",
    "\n",
    "#test_box = gpd.read_file(test_box_path).to_crs(crs)\n",
    "#shorelines_clip = gpd.read_file(os.path.join(data_dir,\"P1\",\"P1_clip_shorelines\"))\n",
    "#transects_clip = gpd.clip(transects_trim,test_box)\n",
    "#transects_clip.to_file(transects_clip_file,driver=\"GeoJSON\")\n",
    "\n",
    "intersections_clip = compute_intersections(transects_clip,shorelines_clip,remove_outliers=True)\n",
    "intersections_clip.to_file(intersections_clip_file,driver=\"GeoJSON\")\n",
    "print(\"Done!\")"
   ]
  }
 ]
}